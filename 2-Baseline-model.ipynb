{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necesary packages\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Basic information\n",
    "    \"AUTHOR\": \"Kiernan\",\n",
    "    \n",
    "    # Training params\n",
    "    \"LR\": 0.001,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"EPOCHS\": 30,\n",
    "    \n",
    "    # Model params\n",
    "    \"FIRST_FILTERS\": 16,\n",
    "    \"CONV_LAYERS\": 4,\n",
    "    \"N_FILTERS\": 8,\n",
    "    \"KERNEL_SIZE\": (3,3),\n",
    "    \"EMBEDDING_SIZE\": 16,\n",
    "    \"DROPOUT\": 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Initialize WANDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mall-off-nothing\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kiern/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kmcguigan/deep-clustering-evaluation/runs/381mmi5r\" target=\"_blank\">smooth-rain-10</a></strong> to <a href=\"https://wandb.ai/kmcguigan/deep-clustering-evaluation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from secrets import WANDB\n",
    "wandb.login(key=WANDB)\n",
    "run = wandb.init(project=\"deep-clustering-evaluation\", entity=\"kmcguigan\", group=\"base-model\", config=config, job_type=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (50000, 28, 28, 1) Val data shape: (10000, 28, 28, 1) Test data shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "(X, y), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X = (X.astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "X = X.reshape((*X.shape, 1))\n",
    "X_test = X_test.reshape((*X_test.shape, 1))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=X_test.shape[0], shuffle=True)\n",
    "print(f\"Train data shape: {X_train.shape} Val data shape: {X_val.shape} Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"body\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 14, 16)        160       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 14, 14, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 8)         1160      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 14, 14, 16)        144       \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 16)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,408\n",
      "Trainable params: 3,312\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_body(image_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=image_shape)\n",
    "    \n",
    "    def conv_block(layer_inputs, n_filters, kernel_size, **kwargs):\n",
    "        x = tf.keras.layers.Conv2D(n_filters, kernel_size, padding=\"same\", **kwargs)(layer_inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        return x\n",
    "    \n",
    "    x = conv_block(inputs, config[\"FIRST_FILTERS\"], config[\"KERNEL_SIZE\"], strides=2)\n",
    "    for _ in range(config[\"CONV_LAYERS\"]):\n",
    "        x = conv_block(x, config[\"N_FILTERS\"], config[\"KERNEL_SIZE\"])\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(config[\"EMBEDDING_SIZE\"], (1,1), padding=\"same\")(x)\n",
    "    outputs = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"body\")\n",
    "\n",
    "body = create_body(X_train.shape[1:])\n",
    "body.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 170\n",
      "Trainable params: 170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_head(n_classes):\n",
    "    inputs = tf.keras.layers.Input(shape=(config[\"EMBEDDING_SIZE\"]))\n",
    "    x = tf.keras.layers.Dropout(config[\"DROPOUT\"])(inputs)\n",
    "    outputs = tf.keras.layers.Dense(n_classes, activation='softmax')(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"head\")\n",
    "\n",
    "head = create_head(10)\n",
    "head.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"combinedModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " body (Functional)           (None, 16)                3408      \n",
      "                                                                 \n",
      " head (Functional)           (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,578\n",
      "Trainable params: 3,482\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    body,\n",
    "    head\n",
    "],\n",
    "    name=\"combinedModel\"\n",
    ")\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=config['LR'])\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),]\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.9502 - sparse_categorical_accuracy: 0.6616 - val_loss: 0.8802 - val_sparse_categorical_accuracy: 0.6524 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.3274 - sparse_categorical_accuracy: 0.9077 - val_loss: 0.3435 - val_sparse_categorical_accuracy: 0.8950 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.2250 - sparse_categorical_accuracy: 0.9340 - val_loss: 0.2887 - val_sparse_categorical_accuracy: 0.9060 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1872 - sparse_categorical_accuracy: 0.9456 - val_loss: 0.1886 - val_sparse_categorical_accuracy: 0.9399 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1644 - sparse_categorical_accuracy: 0.9521 - val_loss: 0.2081 - val_sparse_categorical_accuracy: 0.9388 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1462 - sparse_categorical_accuracy: 0.9575 - val_loss: 0.5320 - val_sparse_categorical_accuracy: 0.8420 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1257 - sparse_categorical_accuracy: 0.9624 - val_loss: 0.1334 - val_sparse_categorical_accuracy: 0.9626 - lr: 5.0000e-04\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1166 - sparse_categorical_accuracy: 0.9655 - val_loss: 0.0927 - val_sparse_categorical_accuracy: 0.9726 - lr: 5.0000e-04\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1144 - sparse_categorical_accuracy: 0.9664 - val_loss: 0.1349 - val_sparse_categorical_accuracy: 0.9570 - lr: 5.0000e-04\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1116 - sparse_categorical_accuracy: 0.9672 - val_loss: 0.1033 - val_sparse_categorical_accuracy: 0.9699 - lr: 5.0000e-04\n",
      "Epoch 11/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1006 - sparse_categorical_accuracy: 0.9706 - val_loss: 0.0756 - val_sparse_categorical_accuracy: 0.9776 - lr: 2.5000e-04\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0989 - sparse_categorical_accuracy: 0.9710 - val_loss: 0.0716 - val_sparse_categorical_accuracy: 0.9789 - lr: 2.5000e-04\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0968 - sparse_categorical_accuracy: 0.9713 - val_loss: 0.0731 - val_sparse_categorical_accuracy: 0.9789 - lr: 2.5000e-04\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0961 - sparse_categorical_accuracy: 0.9718 - val_loss: 0.0752 - val_sparse_categorical_accuracy: 0.9776 - lr: 2.5000e-04\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0907 - sparse_categorical_accuracy: 0.9728 - val_loss: 0.0673 - val_sparse_categorical_accuracy: 0.9810 - lr: 1.2500e-04\n",
      "Epoch 16/30\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.0890 - sparse_categorical_accuracy: 0.9733 - val_loss: 0.0668 - val_sparse_categorical_accuracy: 0.9804 - lr: 1.2500e-04\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0902 - sparse_categorical_accuracy: 0.9730 - val_loss: 0.0697 - val_sparse_categorical_accuracy: 0.9796 - lr: 1.2500e-04\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.0881 - sparse_categorical_accuracy: 0.9733 - val_loss: 0.0662 - val_sparse_categorical_accuracy: 0.9804 - lr: 1.2500e-04\n",
      "Epoch 19/30\n",
      " 325/1563 [=====>........................] - ETA: 19s - loss: 0.0811 - sparse_categorical_accuracy: 0.9753"
     ]
    }
   ],
   "source": [
    "stopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "\n",
    "hist = model.fit(X_train, y_train,\n",
    "                 validation_data=(X_val, y_val),\n",
    "                 batch_size=config[\"BATCH_SIZE\"],\n",
    "                 validation_batch_size=config[\"BATCH_SIZE\"],\n",
    "                 epochs=config[\"EPOCHS\"],\n",
    "                 callbacks=[stopper, lr_reducer, WandbCallback(predictions=8, input_type='images', validation_data=(X_val, y_val))])\n",
    "\n",
    "model.save(os.path.join(run.dir, \"model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12), tight_layout=True)\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(hist.history['sparse_categorical_accuracy'])\n",
    "plt.plot(hist.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Categorical Accuracy')\n",
    "plt.ylabel('Categorical Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(hist.history['lr'])\n",
    "plt.title('Learining Rate')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "run.log({'train_graph': fig})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = model.evaluate(X_test, y_test)\n",
    "run.log({'test/loss':ev[0], 'test/categorical_accuracy':ev[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analyze Test Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "\n",
    "for i in range(16):\n",
    "    run.log({f\"test/image{i}\": wandb.Image(X_test[i,:,:,:], caption=f\"Pred: {preds[i]} Actual: {y_test[i]}\")})\n",
    "\n",
    "cf = confusion_matrix(y_test, np.argmax(preds, axis=-1))\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [x for x in range(0, 10, 1)]\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "sns.heatmap(cf, annot=True, xticklabels=class_names, yticklabels=class_names, cmap='Blues', robust=True)\n",
    "run.log({'confusion_matrix': fig})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
