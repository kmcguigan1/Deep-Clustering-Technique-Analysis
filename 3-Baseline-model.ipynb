{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **A Baseline Model For Cluster Embeddings**\n",
    "\n",
    "This notebook contains the baseline model for cluster embeddings. This is a very simple model, using convolutional layers to transform images into an embeddings space of length N. These embeddings are trained by then using them in a classification loss. This will train the model to produce meaningful embeddings, resulting in good classification scores.\n",
    "\n",
    "## **Notebook Overview**\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1. [Imports & Setup](#Imports-&-Setup)\n",
    "2. [Model Creation](#Model-Creation)\n",
    "3. [Dataset Creation](#Dataset-Creation)\n",
    "4. [Training](#Training)\n",
    "5. [Clustering Embeddings Analysis](#Clustering-Embeddings-Analysis)\n",
    "6. [Volume Of Embeddings Space](#Volume-Of-Embeddings-Space)\n",
    "7. [Collect Cluster Info And Plot Mahalanbois Distance](#Collect-Cluster-Info-And-Plot-Mahalanbois-Distance)\n",
    "8. [Display The Percent Of Volume That Each Cluster Takes Up](#Display-The-Percent-Of-Volume-That-Each-Cluster-Takes-Up)\n",
    "9. [Visualize The Angles Between Points And Clusters](#Visualize-The-Angles-Between-Points-And-Clusters)\n",
    "10. [Visualize The Clustering Embeddings](#Visualize-The-Clustering-Embeddings)\n",
    "\n",
    "## **Imports & Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necesary packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size':14})\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial.distance import mahalanobis, euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is done to limit the memory growth on the GPU\n",
    "# I was having trouble with my GPU memory being filled so I capped the availible memory\n",
    "# growth here.\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=16_384)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configuration Object**\n",
    "\n",
    "This is the configuration object that holds the main parameters we will be using later in our program. If we run our code with Weights and Biases (WANDB) then it will be saved to the given run, allowing us to see the effect of different hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Basic information\n",
    "    \"AUTHOR\": \"Kiernan\",\n",
    "    \n",
    "    # Training params\n",
    "    \"LR\": 0.001,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"EPOCHS\": 50,\n",
    "    \n",
    "    # Model params\n",
    "    \"CONV_LAYERS\": 8,\n",
    "    \"FIRST_FILTERS\": 32,\n",
    "    \"FIRST_KERNEL_SIZE\": (5,5),\n",
    "    \"N_FILTERS\": 16,\n",
    "    \"KERNEL_SIZE\": (3,3),\n",
    "#     \"DENSE_SIZE\": 16,\n",
    "    \"DROPOUT\": 0.2,\n",
    "    \"EMBEDDING_SIZE\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initialize WANDB**\n",
    "\n",
    "Weights and Biases is a very useful machine learning pipelining tool. It can be super handy when it comes to tracking experiments. It is capable of tracking the loss over epochs, the best model, and the parameters used when training different models. All this information can then be used ot optimize the hyper parameters of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_WANDB = False\n",
    "if(RUN_WANDB):\n",
    "    import wandb\n",
    "    from wandb.keras import WandbCallback\n",
    "    from secrets import WANDB\n",
    "    wandb.login(key=WANDB)\n",
    "    run = wandb.init(project=\"deep-clustering-evaluation\", entity=\"kmcguigan\", group=\"base-model\", config=config, job_type=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Data**\n",
    "\n",
    "We created the data ahead of time to make these experiments more reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (50000, 28, 28, 1) Val data shape: (10000, 28, 28, 1) Test data shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "with open('data/train.npy', mode='rb') as infile:\n",
    "    X_train = np.load(infile, allow_pickle=True)\n",
    "    y_train = np.load(infile, allow_pickle=True)\n",
    "\n",
    "with open('data/val.npy', mode='rb') as infile:\n",
    "    X_val = np.load(infile, allow_pickle=True)\n",
    "    y_val = np.load(infile, allow_pickle=True)\n",
    "\n",
    "with open('data/test.npy', mode='rb') as infile:\n",
    "    X_test = np.load(infile, allow_pickle=True)\n",
    "    y_test = np.load(infile, allow_pickle=True)\n",
    "    \n",
    "with open('data/viz.npy', mode='rb') as infile:\n",
    "    X_viz = np.load(infile, allow_pickle=True)\n",
    "    y_viz = np.load(infile, allow_pickle=True)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape} Val data shape: {X_val.shape} Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load in the Labels**\n",
    "\n",
    "This dataframe maps the labels to their corresponding names. For classification we want numeric labels going from 1...N where N is the number of classes we have. In certain scenarios (like on MNIST) the label matches the name, but for other datasets we would want to map the numeric label to the human understandable name we associate with each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  name\n",
       "0      0     0\n",
       "1      1     1\n",
       "2      2     2\n",
       "3      3     3\n",
       "4      4     4\n",
       "5      5     5\n",
       "6      6     6\n",
       "7      7     7\n",
       "8      8     8\n",
       "9      9     9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_csv('data/label_names.csv')\n",
    "labels_df['label'] = labels_df['label'].astype(int)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Creation**\n",
    "\n",
    "### **Create Model Body**\n",
    "\n",
    "The body of the model is a very simple convolutional architecture. After a number of convolutional layers we finish with global average pooling which takes the average of all the pixels in each channel, returning a vector of (Batch Size, Number of Chanels). This vector can now be considered the embedding vector, representing the components of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 16:17:37.433045: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-09 16:17:37.433615: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"body\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 28, 28, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 16)        4624      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 28, 28, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 28, 28, 16)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 28, 28, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 28, 28, 16)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 28, 28, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 28, 28, 16)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 28, 28, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 28, 28, 16)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 28, 28, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_5 (ReLU)              (None, 28, 28, 16)        0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 28, 28, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_6 (ReLU)              (None, 28, 28, 16)        0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 28, 28, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_7 (ReLU)              (None, 28, 28, 16)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 28, 28, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_8 (ReLU)              (None, 28, 28, 16)        0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 3)         435       \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 3)                0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,771\n",
      "Trainable params: 22,451\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_body(image_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=image_shape)\n",
    "    \n",
    "    def cov_block(layer_inputs, n_filters, kernel_size, strides=1):\n",
    "        x = tf.keras.layers.Conv2D(n_filters, kernel_size, padding=\"same\", strides=strides)(layer_inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        return x\n",
    "    \n",
    "    x = cov_block(inputs, config[\"FIRST_FILTERS\"], config[\"FIRST_KERNEL_SIZE\"])\n",
    "    for _ in range(config[\"CONV_LAYERS\"]):\n",
    "        x = cov_block(x, config[\"N_FILTERS\"], config[\"KERNEL_SIZE\"])\n",
    "        \n",
    "    x = tf.keras.layers.Conv2D(config[\"EMBEDDING_SIZE\"], config[\"KERNEL_SIZE\"], padding=\"same\")(x)\n",
    "    outputs = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"body\")\n",
    "\n",
    "body = create_body(X_train.shape[1:])\n",
    "body.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create the Model Head**\n",
    "\n",
    "The head is what makes the actual predictions. A good way to think about this is that the body learns to extract the features from the images, then the head uses these extracted features to classify which number is represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                40        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40\n",
      "Trainable params: 40\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_head(n_classes):\n",
    "    inputs = tf.keras.layers.Input(shape=(config[\"EMBEDDING_SIZE\"]))\n",
    "#     x = tf.keras.layers.Dropout(config[\"DROPOUT\"])(inputs)\n",
    "    outputs = tf.keras.layers.Dense(n_classes, activation='softmax')(inputs)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"head\")\n",
    "\n",
    "head = create_head(10)\n",
    "head.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create the Overall Model**\n",
    "\n",
    "Here we combine the body and the head to a full on model, we attatch the loss function and this gives us our trainable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"combinedModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " body (Functional)           (None, 3)                 22771     \n",
      "                                                                 \n",
      " head (Functional)           (None, 10)                40        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,811\n",
      "Trainable params: 22,491\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    body,\n",
    "    head\n",
    "],\n",
    "    name=\"combinedModel\"\n",
    ")\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=config['LR'])\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),]\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**\n",
    "\n",
    "Here we train the model, this uses both early stopping and a learning rate reducer to prevent overfitting and help us get the best possible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 16:17:37.985432: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-12-09 16:17:38.608650: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5991 - sparse_categorical_accuracy: 0.8034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 16:18:09.493909: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 34s 21ms/step - loss: 0.5991 - sparse_categorical_accuracy: 0.8034 - val_loss: 0.5919 - val_sparse_categorical_accuracy: 0.8174 - lr: 0.0010\n",
      "Epoch 2/50\n",
      " 307/1563 [====>.........................] - ETA: 23s - loss: 0.2520 - sparse_categorical_accuracy: 0.9255"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
    "if(RUN_WANDB):\n",
    "    callbacks = [stopper, lr_reducer, WandbCallback(predictions=8, input_type='images', validation_data=(X_val, y_val))]\n",
    "else:\n",
    "    callbacks = [stopper, lr_reducer]\n",
    "\n",
    "hist = model.fit(X_train, y_train,\n",
    "                 validation_data=(X_val, y_val),\n",
    "                 batch_size=config[\"BATCH_SIZE\"],\n",
    "                 validation_batch_size=config[\"BATCH_SIZE\"],\n",
    "                 epochs=config[\"EPOCHS\"],\n",
    "                 shuffle=True,\n",
    "                 callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vizualize Training Results**\n",
    "\n",
    "This shows how well the model learned what it intended to. We can see how the leraning rate was reduced, the accuracy of predictions, and the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12), tight_layout=True)\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(hist.history['sparse_categorical_accuracy'])\n",
    "plt.plot(hist.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Categorical Accuracy')\n",
    "plt.ylabel('Categorical Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(hist.history['lr'])\n",
    "plt.title('Learining Rate')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "if(RUN_WANDB):\n",
    "    run.log({'train_graph': wandb.Image(fig, caption=\"Training Graph\")})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analyze Results on the Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = model.evaluate(X_test, y_test)\n",
    "if(RUN_WANDB):\n",
    "    run.log({'test/loss':ev[0], 'test/categorical_accuracy':ev[1]})\n",
    "ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clustering Embeddings Analysis**\n",
    "\n",
    "Here we will use kmeans to create clusters of the embeddings. Since we know the number of labels we will use this as our K-value (this is the number of clusters). We then go through each cluster and find the dominant label in this cluster. We make the assumption here that the network performed half decent and that most samples are correctly labelled. For the purposes of this project, which is to show off the effectiveness of different clustering techniques, this works fine. For different projects it would be advisable to not make this assumption, maybe classifying samples with a K-Nearest Neighbors classifier based on the labelled training data.\n",
    "\n",
    "Cluster accuracy represents the fraction of samples that are in the correct cluster, or in other words the number of samples that are found in the cluster with a matching dominant label. We can then create a confusion matrix showing the actual and predicted class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster_accuracy(embeddings, y):\n",
    "    # run k-means\n",
    "    all_labels = labels_df['label'].values\n",
    "    kmeans = KMeans(n_clusters=len(all_labels), random_state=123)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # find the mapping of each kmeans label to the actual label\n",
    "    label_mappings = {}\n",
    "    for label in all_labels:\n",
    "        values, counts = np.unique(y[np.where(labels==label)], return_counts=True)\n",
    "        label_mappings[label] = values[np.argmax(counts)]\n",
    "    \n",
    "    # map kmeans labels to actual label\n",
    "    map_labels = np.vectorize(lambda x: label_mappings[x])\n",
    "    mapped_labels = map_labels(labels)\n",
    "    \n",
    "    # calculate the centers for each mapped labels\n",
    "    kmeans_clusters = kmeans.cluster_centers_\n",
    "    mapped_clusters = {}\n",
    "    for label in all_labels:\n",
    "        mapped_clusters[label_mappings[label]] = kmeans_clusters[label,:]\n",
    "    return accuracy_score(y.reshape((-1,1)), mapped_labels.reshape((-1,1))), mapped_labels, mapped_clusters\n",
    "\n",
    "embeddings = body.predict(X_test)\n",
    "accuracy, preds, cluster_centers = kmeans_cluster_accuracy(embeddings, y_test)\n",
    "# run.log({'test/clustering_accuracy':accuracy})\n",
    "print(f\"Cluster Accurancy {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cf = confusion_matrix(y_test, preds, normalize='true')\n",
    "class_names = labels_df['name'].values\n",
    "fig, ax = plt.subplots(1,1,figsize=(24,16))\n",
    "sns.heatmap(cf, annot=True, xticklabels=class_names, yticklabels=class_names, cmap='Blues', robust=True, ax=ax, annot_kws={\"size\": 14})\n",
    "ax.set_ylabel(\"Actual Label\", fontdict={'fontsize':16})\n",
    "ax.set_xlabel(\"Predicted Label\", fontdict={'fontsize':16})\n",
    "if(RUN_WANDB):\n",
    "    run.log({'confusion_matrix': wandb.Image(fig, caption=\"Confusion Matrix\")})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(RUN_WANDB):\n",
    "    run.log({\n",
    "        \"testExamples\": [wandb.Image(im, caption=f\"Pred:{pred} Lable:{y}\")\n",
    "                            for im, pred, y in zip(X_test[:16,:,:,:], np.argmax(preds[:16], axis=-1), y_test[:16])]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Volume Of Embeddings Space**\n",
    "\n",
    "First we will get the volume of the overall embeddings space. This will help us get a baseline for the total space that was learned here. This will make it easier to compare embeddings later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull\n",
    "hull = ConvexHull(embeddings)\n",
    "overall_volume = hull.volume\n",
    "overall_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Collect Cluster Info And Plot Mahalanbois Distance**\n",
    "\n",
    "Here we calculated the covariance matrix for the different clusters. This will let us calculate the Mahalanbois distance for the positive and negative points in the cluster. Since our embeddings space is not 1-dimensional our clusters are not 1-dimensional either. Since our clusters may vary differently on the different basis vectors a simple euclidean distance might not make sense. Picture an oval with semi-axis 10 on the x-axis and 5 on the y-axis. A point 10 units away on the x-axis is closer to the mass of the oval than a point 10 units away on the y-axis. This is where Mahalanbois distance comes in. This gives us the covariance adjusted distance from the cluster. Letting us easily compare the true positives and false positives classification within a cluster.\n",
    "\n",
    "\n",
    "#### **Calculating the Volume of a Cluster**\n",
    "The next thing we do is we also calculate the volume of the embeddings for the cluster. This is done in two ways. The first is simple and we get the volume of all points in the cluster. This is fine but a thing to note is that there will always be outlier points, and maybe we don't want to worry about those few outliers so much. Well we have a covariance matrix which tells us the covariance and the correlation between covariance in the different dimensions. \n",
    "\n",
    "We will start by thinking of the covariance matrix as some linear transformation from standard basis vecotrs. Now unless the correlation between the different dimensions is independent, meaning we have zeros for any cells off the diagonal of the covariance matrix, this linear transformation is throwing the basis vectors off their span making them uselss to calculate the volume of the ellipsoid. You can already probably guess that this is where the eigen vectors and eigen values come in. The eigenvectors will be the new basis vectors we consider as they aren't thrown off their span, and the eigen-values will be the scalling factor each of these basis vectors undergo. Since we want volume, and don't care so much about orientation we can just pretend we have an axis aligned error ellipsoid with the semi-axis being the square root (since variance is the squared standard deviation) of the eigen-values.\n",
    "\n",
    "With the standard deviation in each direction, and new basis vectors that make the dimensions of our error ellisoid independent from one another, we can write the error ellipsoid like we would any standard axis-aligned ellipsoid like (x / sqrt(eigen-value-1))^2 + (y / sqrt(eigen-value-2))^2 + ... = S where is some scalling factor. This is handy becuase based on the covariance matrix we know the values of say x are normally distributed, we also subtracted the centroid when calculating this so the mean of this distribution is zero. Since we are dividing by the sqrt of the eigen value, this is the same thing as dividing by the standard deviation, which gives us a standard normal random vairable. \n",
    "\n",
    "Becuase we take the square of this standard normal random variable and add them together this is the difinition of a chi-squared distribution. Since the degrees of freedom is just the number of dimensions, we can use the lookup table to find the chi-squared critical value for n degrees of freedom and a 95% confidence level. \n",
    "\n",
    "A question I had myself was why this equation works for different sizes of error ellipsoids, the reason is that we have standardized normal random variables, we are accounting for the size of the independent random normal variables. This is why the scalling factor from the chi-squared distribution is the same for a given degrees of freedom and confidence level. I find it cool how all these things relate together.\n",
    "\n",
    "At this point you may or may not be reading this explanation, I don't blame you either way. The fact you're reading this part means you're still with me so here we go. We have this cool formula but now how do we get the semi-axis lengths? Well this is the easy part, essentially we want the semi-axis lengths when all other semi-axis are zero. In other words the axis aligned ellipse semi-axis length on the x dimension is the length when the length on the y dimension is 0. Essentially the formula looks like this semi-axis-length = sqrt(S) * sqrt(eigen-value). Pretty simple right?\n",
    "\n",
    "https://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/\n",
    "\n",
    "https://cookierobotics.com/007/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PLOT = True\n",
    "\n",
    "def get_conf_interval(distances, indecies=None):\n",
    "    if(indecies is not None):\n",
    "        distances = distances[indecies]\n",
    "    conf_interval = [np.min(distances),]\n",
    "    for inter in [2,5,25,75,95,98]:\n",
    "        conf_interval.append(np.percentile(distances,inter))\n",
    "    conf_interval.append(np.max(distances))\n",
    "    return conf_interval\n",
    "\n",
    "def cosine_similarity(samples, point):\n",
    "    samples = samples / np.linalg.norm(samples, axis=-1).reshape(-1,1)\n",
    "    point = point / np.linalg.norm(point)\n",
    "    cosine = samples @ point\n",
    "    cosine = np.clip(cosine, -1.0, 1.0)\n",
    "    theta = np.arccos(cosine)\n",
    "    return theta\n",
    "    \n",
    "cluster_info = {}\n",
    "for cluster in labels_df['label'].values:\n",
    "    # pull the data we work with for this cluster\n",
    "    indecies = np.where(preds==cluster)[0]\n",
    "    samples = embeddings[indecies]\n",
    "    actuals = y_test[indecies]\n",
    "    centroid = cluster_centers[cluster]\n",
    "\n",
    "    # grab the indecies of both positive and negative elements in the cluster\n",
    "    positive_indecies = np.where(actuals==cluster)[0]\n",
    "    negative_indecies = np.where(actuals!=cluster)[0]\n",
    "    \n",
    "    # calculate the covariance matrix of all samples\n",
    "    cov = np.cov((samples - centroid).T)\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    \n",
    "    # calculate sample distances\n",
    "    sample_mahalanbois_distances = np.array([mahalanobis(centroid, samples[idx,:], inv_cov) for idx in range(samples.shape[0])])\n",
    "    sample_theta_distances = cosine_similarity(samples, centroid)\n",
    "    \n",
    "    # calculate the confidence interval for point densities\n",
    "    conf_interval_mahalanobis = get_conf_interval(sample_mahalanbois_distances)\n",
    "    conf_interval_theta = get_conf_interval(sample_theta_distances)\n",
    "    \n",
    "    # calculate the volume of the covariance matrix\n",
    "    hull = ConvexHull(samples)\n",
    "    volume = hull.volume\n",
    "    \n",
    "    eigen_values, eigen_vectors = np.linalg.eig(cov)\n",
    "    if(config[\"EMBEDDING_SIZE\"]==3):\n",
    "        get_side_length = np.vectorize(lambda x: np.sqrt(7.815*x))\n",
    "        semi_axes_lengths = get_side_length(eigen_values)\n",
    "        confidence_volume = (4 / 3) * np.pi * semi_axes_lengths[0] * semi_axes_lengths[1] * semi_axes_lengths[2]\n",
    "    elif(config[\"EMBEDDING_SIZE\"]==2):\n",
    "        get_side_length = np.vectorize(lambda x: np.sqrt(5.991*x))\n",
    "        semi_axes_lengths = get_side_length(eigen_values)\n",
    "        confidence_volume = np.pi * semi_axes_lengths[0] * semi_axes_lengths[1]\n",
    "    else: \n",
    "        raise Exception(f'Invalid dimensions {len(semi_axes_lengths)}')\n",
    "    \n",
    "    # record the cluster information\n",
    "    cluster_info[cluster] = {\n",
    "        'cov':cov,\n",
    "        'inv_cov':inv_cov,\n",
    "        'conf_interval_mahalanobis':conf_interval_mahalanobis,\n",
    "        'conf_interval_theta':conf_interval_theta,\n",
    "        'positive_examples':len(positive_indecies),\n",
    "        'negative_examples':len(negative_indecies),\n",
    "        'volume':volume,\n",
    "        'confidence_volume':confidence_volume,\n",
    "    }\n",
    "    \n",
    "    if(PLOT):\n",
    "        # plot the distances\n",
    "        fig, ax = plt.subplots(1,1,figsize=(22,12))\n",
    "        ax.set_title(f'Cluster {cluster} Mahalanobis Distances', fontsize=24)\n",
    "        # create the dataframe to show or properly label the plots\n",
    "        g1 = sns.histplot(data=sample_mahalanbois_distances[positive_indecies], stat='percent', color='mediumblue', kde=True, ax=ax, bins=20)\n",
    "        g2 = sns.histplot(data=sample_mahalanbois_distances[negative_indecies], stat='percent', color='orange', kde=True, ax=ax, bins=20)\n",
    "        # update the labels for the legend\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='mediumblue',edgecolor='blue',label='Correctly Labelled Points'),\n",
    "            Patch(facecolor='orange',edgecolor='red',label='Incorrectly Labelled Points'),\n",
    "        ]\n",
    "        plt.legend(handles=legend_elements, loc='best', fontsize=20)\n",
    "        ax.set_xlim(left=-0.001)\n",
    "        plt.show()\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Collect Cluster-to-Cluster Analysis Information**\n",
    "\n",
    "Here we want to get some information about the other clusters. There are a lot of statistics that are calculated and not used such as all the confidence intervals on distances. I left the code here as some ideas for more ways to dive into the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_cluster = {}\n",
    "for cluster in labels_df['label'].values:\n",
    "    # pull the data we work with for this cluster\n",
    "    indecies = np.where(preds==cluster)[0]\n",
    "    samples = embeddings[indecies]\n",
    "    actuals = y_test[indecies]\n",
    "    centroid = cluster_centers[cluster]\n",
    "    \n",
    "    # iterate over other clusters to see how these points stack up on them\n",
    "    for compare_cluster in labels_df['label'].values:\n",
    "        if(cluster == compare_cluster):\n",
    "            continue\n",
    "            \n",
    "        # get the cluster we will compare against\n",
    "        comparative_centroid = cluster_centers[compare_cluster]\n",
    "        \n",
    "        # get the distance between the comparative centroid and this one\n",
    "        centroid_distance_mahalanbois = mahalanobis(centroid, comparative_centroid, cluster_info[cluster]['inv_cov'])\n",
    "        centroid_distance_theta = cosine_similarity(centroid.reshape(1,-1), comparative_centroid)\n",
    "        \n",
    "        # get the distance between points in this cluster\n",
    "        sample_mahalanbois_distances = np.array([mahalanobis(comparative_centroid, samples[idx,:], cluster_info[compare_cluster]['inv_cov']) for idx in range(samples.shape[0])])\n",
    "        sample_theta_distances = cosine_similarity(samples, comparative_centroid)\n",
    "    \n",
    "        # grab the indecies of both positive and negative elements in the cluster\n",
    "        positive_indecies = np.where(actuals==compare_cluster)[0]\n",
    "        negative_indecies = np.where(actuals!=compare_cluster)[0]\n",
    "        \n",
    "        # calculate the confidence interval for point densities\n",
    "        conf_interval_mahalanobis = get_conf_interval(sample_mahalanbois_distances)\n",
    "        conf_interval_theta = get_conf_interval(sample_theta_distances)\n",
    "        \n",
    "        # record what we learned\n",
    "        d = cluster_to_cluster.get(compare_cluster, dict())\n",
    "        d[cluster] = {\n",
    "            'centroid_distance_mahalanbois':centroid_distance_mahalanbois,\n",
    "            'centroid_distance_theta':centroid_distance_theta,\n",
    "            'conf_interval_mahalanobis':conf_interval_mahalanobis,\n",
    "            'conf_interval_theta':conf_interval_theta,\n",
    "            'positive_samples':len(positive_indecies),\n",
    "            'negative_samples':len(negative_indecies)\n",
    "        }\n",
    "        cluster_to_cluster[compare_cluster] = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Display The Percent Of Volume That Each Cluster Takes Up**\n",
    "\n",
    "Here we display the overall and 95% confidence level volumes that the clusters take up. This is a good way of showing how tight the clusters are compared to the space aloted. This is not a perfect measure but we can look at the differences between the overall and confidence level volumes. We might also be interested in the percent of the total volume that is covered by points. The tigher the clusters the lower the volume percent. This will be skewed if the overall volume is smaller, but using the 95% to total comparison we can get an idea for the ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes = []\n",
    "confidence_volumes = []\n",
    "for cluster in cluster_info.keys():\n",
    "    volume_ratio = (cluster_info[cluster]['volume'] / overall_volume) * 100\n",
    "    confidence_volume_ratio = (cluster_info[cluster]['confidence_volume'] / overall_volume) * 100\n",
    "    volumes.append(volume_ratio)\n",
    "    confidence_volumes.append(confidence_volume_ratio)\n",
    "    print(f\"Cluster {cluster} => Volume Ratio: {volume_ratio:,.3f}% 95% Confidence Volume Ratio: {confidence_volume_ratio:,.3f}%\")\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Overall Statistics\")\n",
    "print(f\"Mean Values   => Mean Volume Ratio: {np.mean(volumes):,.3f}% Mean 95% Confidence Volume Ratio: {np.mean(confidence_volumes):,.3f}%\")\n",
    "print(f\"Summed Values => Total Volume Ratio: {np.sum(volumes):,.3f}% Total 95% Confidence Volume Ratio: {np.sum(confidence_volumes):,.3f}%\")\n",
    "print(\"\")\n",
    "print(f\"Total Volume: {overall_volume:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualize The Angles Between Points And Clusters**\n",
    "\n",
    "Here is the important visualization. For each cluster we find the nearest cluster (given the angular distance between centroids) and we plot the angle between the points of the given cluster to the nearest cluster and vise-versa. This is one that is size agnostic, we can see the tighness of the clusters. The more centered the clusters the better the embeddings are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cluster in cluster_to_cluster.keys():\n",
    "    # get the samples\n",
    "    indecies = np.where(preds==cluster)[0]\n",
    "    samples = embeddings[indecies]\n",
    "    # find the closest cluster to this one\n",
    "    closest_cluster, distance = None, None\n",
    "    for other_cluster in cluster_to_cluster[cluster].keys():\n",
    "        if(distance is None or cluster_to_cluster[cluster][other_cluster]['centroid_distance_theta']<distance):\n",
    "            closest_cluster = other_cluster\n",
    "            distance = cluster_to_cluster[cluster][other_cluster]['centroid_distance_theta']\n",
    "    # get the samples for the closest cluster\n",
    "    closest_indecies = np.where(preds==closest_cluster)[0]\n",
    "    closest_samples = embeddings[closest_indecies]\n",
    "    # get the angular distances\n",
    "    this_to_this_theta = cosine_similarity(samples, cluster_centers[cluster])\n",
    "    this_to_other_theta = cosine_similarity(samples, cluster_centers[closest_cluster])\n",
    "    other_to_this_theta = cosine_similarity(closest_samples, cluster_centers[cluster])\n",
    "    other_to_other_theta = cosine_similarity(closest_samples, cluster_centers[closest_cluster])\n",
    "    # plot the angular distance between points\n",
    "    _, ax = plt.subplots(1,1,figsize=(22,8))\n",
    "    ax.scatter(this_to_this_theta, this_to_other_theta, color='blue', label=f'Samples from Cluster {cluster}', alpha=0.5)\n",
    "    ax.scatter(other_to_this_theta, other_to_other_theta, color='red', label=f'Samples from Cluster {closest_cluster}', alpha=0.5)\n",
    "    ax.set_title(f'Comparison of Angles Between Cluster Points of Cluster {cluster} & Cluster {closest_cluster}', fontdict={'fontsize':24})\n",
    "    ax.set_xlabel(f'Theta Between Point and {cluster} Cluster', fontsize=18)\n",
    "    ax.set_ylabel(f'Theta Between Point and {closest_cluster} Cluster', fontsize=18)\n",
    "    ax.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualize The Clustering Embeddings**\n",
    "\n",
    "Here we try and see what the clustering looks like. I visualize this in both 2D and 3D. The reason for this is that TSNE is good at keeping items that are close in a higher dimension close in a lower one, but it isint always good when it comes to shapes and when comparing relative distances. What I mean by relative distances is say there is a point x on the edge of cluster A, the distance A -> x in TSNE cannot be directly compared to say the distance of point y on edge of cluster B since these distances can be manipulated in the transform.\n",
    "\n",
    "A detailed TSNE Overview\n",
    "\n",
    "https://distill.pub/2016/misread-tsne/\n",
    "\n",
    "I found the code to make the very nice visualizations here\n",
    "\n",
    "https://towardsdatascience.com/visualizing-feature-vectors-embeddings-using-pca-and-t-sne-ef157cea3a42#:~:text=t%2Ddistributed%20stochastic%20neighbour%20embedding,non%2Dlinear%20dimensionality%20reduction%20technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings_2d(embeddings, y):\n",
    "    if(embeddings.shape[1] != 2):\n",
    "        tsne = TSNE(n_components=2, learning_rate='auto', init='random', verbose=1, perplexity=50)\n",
    "        embeddings = tsne.fit_transform(embeddings)\n",
    "    cmap = cm.get_cmap('tab20')\n",
    "    fig, ax = plt.subplots(figsize=(22,12))\n",
    "    num_categories = 10\n",
    "    for lab in range(num_categories):\n",
    "        indices = y==lab\n",
    "        ax.scatter(\n",
    "            embeddings[indices,0],\n",
    "            embeddings[indices,1], \n",
    "            c=np.array(cmap(lab)).reshape(1,4), \n",
    "            label=labels_df['name'].values[lab], \n",
    "            alpha=0.5\n",
    "        )\n",
    "    ax.legend(fontsize='large', markerscale=2)\n",
    "    if(RUN_WANDB):\n",
    "        run.log({'tsne_embeddings': wandb.Image(fig, caption=\"Test Embeddings\")})\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_embeddings_3d(embeddings, y):\n",
    "    if(embeddings.shape[1] != 3):\n",
    "        tsne = TSNE(n_components=3, learning_rate='auto', init='random', verbose=1)\n",
    "        embeddings = tsne.fit_transform(embeddings)\n",
    "    cmap = cm.get_cmap('tab20')\n",
    "    fig = plt.figure(figsize=(22,12))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    num_categories = 10\n",
    "    for lab in range(num_categories):\n",
    "        indices = y == lab\n",
    "        ax.scatter(embeddings[indices, 0],\n",
    "                   embeddings[indices, 1],\n",
    "                   embeddings[indices, 2],\n",
    "                   c=np.array(cmap(lab)).reshape(1, 4),\n",
    "                   label=lab,\n",
    "                   alpha=0.5)\n",
    "    ax.legend(fontsize='large', markerscale=2)\n",
    "    if(RUN_WANDB):\n",
    "        run.log({'tsne3d_embeddings': wandb.Image(fig, caption=\"Test Embeddings 3D\")})\n",
    "    plt.show()\n",
    "\n",
    "embeddings = body.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "visualize_embeddings_2d(embeddings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "visualize_embeddings_3d(embeddings, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Finish the Run and Upload the Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(RUN_WANDB):\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extra Code Used to Calculate Extra Metrics**\n",
    "\n",
    "This code is not used in this analysis, but I kept it here to calculate things like the average euclidean distance between points or the average angular distance between points as a keras callback for during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def angular_distances(embeddings):\n",
    "#     embeddings = tf.math.l2_normalize(embeddings, axis=-1)\n",
    "#     cos = tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "#     cos = K.clip(cos, -1.0, 1.0)\n",
    "#     theta = tf.acos(cos)\n",
    "#     theta = tf.linalg.set_diag(theta, tf.zeros(tf.shape(embeddings)[0]))\n",
    "#     return theta\n",
    "    \n",
    "# def euclidean_distances(embeddings):\n",
    "#     samples, dims = tf.shape(embeddings)\n",
    "#     euclidean_distances = []\n",
    "#     for sample_idx in range(samples):\n",
    "#         euclidean_distances.append(tf.linalg.norm(embeddings - embeddings[sample_idx,:], axis=-1))\n",
    "#     euclidean_distances = tf.stack(euclidean_distances)\n",
    "#     euclidean_distances = tf.linalg.set_diag(euclidean_distances, tf.zeros(tf.shape(euclidean_distances)[0]))\n",
    "#     return euclidean_distances\n",
    "\n",
    "# def apply_metric(embeddings, labels, metric):\n",
    "#     # reshape the labels and find where they match versus the transposed version\n",
    "#     labels = tf.reshape(labels, (-1,1))\n",
    "#     adj = tf.equal(labels, tf.transpose(labels))\n",
    "#     adj_not = tf.math.logical_not(adj)\n",
    "#     # remove self from the adj list\n",
    "#     diag = tf.zeros([tf.shape(labels)[0]])\n",
    "#     diag = tf.cast(diag, tf.bool)\n",
    "#     adj = tf.linalg.set_diag(adj, diag)\n",
    "#     # caluclate the distances between points\n",
    "#     distances = metric(embeddings)\n",
    "#     pos_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(distances, mask=adj))\n",
    "#     neg_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(distances, mask=adj_not))\n",
    "#     # calculate the distances between points for each label\n",
    "#     label_metrics = {}\n",
    "#     for label in labels_df['label'].values:\n",
    "#         # get the indecies for this label\n",
    "#         indecies = np.where(labels==label)[0]\n",
    "#         # get the label mask\n",
    "#         label_mask = tf.equal(labels, label)\n",
    "#         label_mask = tf.repeat(label_mask, tf.shape(label_mask)[0], axis=-1)\n",
    "#         # get the label distance means\n",
    "#         label_pos_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(distances, mask=tf.math.logical_and(adj, label_mask)))\n",
    "#         label_neg_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(distances, mask=tf.math.logical_and(adj_not, label_mask)))\n",
    "#         # record the label distance means\n",
    "#         label_metrics[label] = (label_pos_dist_mean, label_neg_dist_mean)\n",
    "#     return pos_dist_mean, neg_dist_mean, label_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_dist_mean, neg_dist_mean, label_metrics = apply_metric(embeddings, y_test, angular_distances)\n",
    "# print(f\"Positive Angular Distances: {pos_dist_mean}\")\n",
    "# print(f\"Negative Angular Distances: {neg_dist_mean}\")\n",
    "# if(RUN_WANDB):\n",
    "#     run.log({'test/positive_angular_distances':pos_dist_mean,\"test/negative_angular_distances\":neg_dist_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label, (pos_dist_mean, neg_dist_mean) in label_metrics.items():\n",
    "#     print(f\"{label} Positive Angular Distances: {pos_dist_mean}\")\n",
    "#     print(f\"{label} Negative Angular Distances: {neg_dist_mean}\")\n",
    "#     print(\"\")\n",
    "#     if(RUN_WANDB):\n",
    "#         run.log({f'test/{label}_positive_angular_distances':pos_dist_mean,f\"test/{label}_negative_angular_distances\":neg_dist_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_dist_mean, neg_dist_mean, label_metrics = apply_metric(embeddings, y_test, euclidean_distances)\n",
    "# print(f\"Positive Euclidean Distances: {pos_dist_mean}\")\n",
    "# print(f\"Negative Euclidean Distances: {neg_dist_mean}\")\n",
    "# if(RUN_WANDB):\n",
    "#     run.log({'test/positive_euclidean_distances':pos_dist_mean,\"test/negative_euclidean_distances\":neg_dist_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label, (pos_dist_mean, neg_dist_mean) in label_metrics.items():\n",
    "#     print(f\"{label} Positive Euclidean Distances: {pos_dist_mean}\")\n",
    "#     print(f\"{label} Negative Euclidean Distances: {neg_dist_mean}\")\n",
    "#     print(\"\")\n",
    "#     if(RUN_WANDB):\n",
    "#         run.log({f'test/{label}_positive_euclidean_distances':pos_dist_mean,f\"test/{label}_negative_euclidean_distances\":neg_dist_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_conf_interval(distances, indecies=None):\n",
    "#     if(indecies is not None):\n",
    "#         distances = distances[indecies]\n",
    "#     conf_interval = [np.min(distances),]\n",
    "#     for inter in [2,5,25,75,95,98]:\n",
    "#         conf_interval.append(np.percentile(distances,inter))\n",
    "#     conf_interval.append(np.max(distances))\n",
    "#     return conf_interval\n",
    "\n",
    "# cluster_info = {}\n",
    "# for cluster in labels_df['label'].values:\n",
    "#     # pull the data we work with for this cluster\n",
    "#     indecies = np.where(preds==cluster)[0]\n",
    "#     samples = embeddings[indecies]\n",
    "#     actuals = y_test[indecies]\n",
    "#     centroid = cluster_centers[cluster]\n",
    "\n",
    "#     # grab the indecies of both positive and negative elements in the cluster\n",
    "#     positive_indecies = np.where(actuals==cluster)[0]\n",
    "#     negative_indecies = np.where(actuals!=cluster)[0]\n",
    "    \n",
    "#     # calculate the covariance matrix of all samples\n",
    "# #     cov = np.cov((samples - centroid).T)\n",
    "#     cov = np.cov(samples.T)\n",
    "#     inv_cov = np.linalg.inv(cov)\n",
    "    \n",
    "#     # calculate sample distances\n",
    "# #     sample_distances = np.array([euclidean(centroid, samples[idx,:]) for idx in range(samples.shape[0])])\n",
    "#     sample_mahalanbois_distances = np.array([mahalanobis(centroid, samples[idx,:], inv_cov) for idx in range(samples.shape[0])])\n",
    "    \n",
    "# #     # calculate the confidence interval for point densities\n",
    "#     conf_interval_mahalanobis = get_conf_interval(sample_mahalanbois_distances)\n",
    "# #     negative_conf_interval_mahalanobis = get_conf_interval(sample_mahalanbois_distances, indecies=negative_indecies)\n",
    "# #     positive_conf_interval_mahalanobis = get_conf_interval(sample_mahalanbois_distances, indecies=positive_indecies)\n",
    "    \n",
    "# #     conf_interval = get_conf_interval(sample_distances)\n",
    "# #     negative_conf_interval = get_conf_interval(sample_distances, indecies=negative_indecies)\n",
    "# #     positive_conf_interval = get_conf_interval(sample_distances, indecies=positive_indecies)\n",
    "    \n",
    "#     # record the cluster information\n",
    "#     cluster_info[cluster] = {\n",
    "#         'cov':cov,\n",
    "#         'inv_cov':inv_cov,\n",
    "#         'conf_interval_mahalanobis':conf_interval_mahalanobis,\n",
    "# #         'positive_conf_interval_mahalanobis':positive_conf_interval_mahalanobis,\n",
    "# #         'negative_conf_interval_mahalanobis':negative_conf_interval_mahalanobis,\n",
    "# #         'conf_interval':conf_interval,\n",
    "# #         'positive_conf_interval':positive_conf_interval,\n",
    "# #         'negative_conf_interval':negative_conf_interval,\n",
    "#         'positive_examples':len(positive_indecies),\n",
    "#         'negative_examples':len(negative_indecies)\n",
    "#     }\n",
    "    \n",
    "#     # plot the distances\n",
    "#     fig, ax = plt.subplots(1,1,figsize=(22,12))\n",
    "#     ax.set_title(f'Cluster {cluster} Mahalanobis Distances', fontsize=24)\n",
    "#     # create the dataframe to show or properly label the plots\n",
    "#     g1 = sns.histplot(data=sample_mahalanbois_distances[positive_indecies], stat='percent', color='mediumblue', kde=True, ax=ax, bins=20)\n",
    "#     g2 = sns.histplot(data=sample_mahalanbois_distances[negative_indecies], stat='percent', color='orange', kde=True, ax=ax, bins=20)\n",
    "#     # update the labels for the legend\n",
    "#     legend_elements = [\n",
    "#         Patch(facecolor='mediumblue',edgecolor='blue',label='Correctly Labelled Points'),\n",
    "#         Patch(facecolor='orange',edgecolor='red',label='Incorrectly Labelled Points'),\n",
    "#     ]\n",
    "#     plt.legend(handles=legend_elements, loc='best', fontsize=20)\n",
    "#     plt.show()\n",
    "#     print(\"\")\n",
    "\n",
    "\n",
    "# def positive_cluster_analysis(distances, indecies):\n",
    "#     if(len(indecies)==0):\n",
    "#         return (np.nan,np.nan,np.nan)\n",
    "#     return (\n",
    "#         np.min(distances[indecies]),\n",
    "#         np.mean(distances[indecies]),\n",
    "#         np.max(distances[indecies]),\n",
    "#     )\n",
    "\n",
    "# cluster_to_cluster = {}\n",
    "# for cluster in labels_df['label'].values:\n",
    "#     # pull the data we work with for this cluster\n",
    "#     indecies = np.where(preds==cluster)[0]\n",
    "#     samples = embeddings[indecies]\n",
    "#     actuals = y_test[indecies]\n",
    "#     centroid = cluster_centers[cluster]\n",
    "#     # iterate over other clusters to see how these points stack up on them\n",
    "#     for compare_cluster in labels_df['label'].values:\n",
    "#         if(cluster == compare_cluster):\n",
    "#             continue\n",
    "            \n",
    "#         # get the cluster we will compare against\n",
    "#         comparative_centroid = cluster_centers[compare_cluster]\n",
    "        \n",
    "#         # get the distance between the comparative centroid and this one\n",
    "#         centroid_distance_mahalanbois = mahalanobis(comparative_centroid, centroid, cluster_info[compare_cluster]['inv_cov'])\n",
    "        \n",
    "#         # get the distance between points in this cluster\n",
    "# #         sample_distances = np.array([euclidean(comparative_centroid, samples[idx,:]) for idx in range(samples.shape[0])])\n",
    "#         sample_mahalanbois_distances = np.array([mahalanobis(comparative_centroid, samples[idx,:], cluster_info[compare_cluster]['inv_cov']) for idx in range(samples.shape[0])])\n",
    "    \n",
    "#         # grab the indecies of both positive and negative elements in the cluster\n",
    "#         positive_indecies = np.where(actuals==compare_cluster)[0]\n",
    "#         negative_indecies = np.where(actuals!=compare_cluster)[0]\n",
    "        \n",
    "#         # calculate the confidence interval for point densities\n",
    "#         conf_interval_mahalanobis = get_conf_interval(sample_mahalanbois_distances)\n",
    "# #         negative_conf_interval_mahalanobis = get_conf_interval(sample_mahalanbois_distances, indecies=negative_indecies)\n",
    "# #         positive_conf_interval_mahalanobis = positive_cluster_analysis(sample_mahalanbois_distances, positive_indecies)\n",
    "        \n",
    "# #         conf_interval = get_conf_interval(sample_distances)\n",
    "# #         negative_conf_interval = get_conf_interval(sample_distances, indecies=negative_indecies)\n",
    "# #         positive_conf_interval = positive_cluster_analysis(sample_distances, positive_indecies)\n",
    "        \n",
    "#         # record what we learned\n",
    "#         d = cluster_to_cluster.get(compare_cluster, dict())\n",
    "#         d[cluster] = {\n",
    "# #             'centroid_distance':centroid_distance,\n",
    "#             'centroid_distance_mahalanbois':centroid_distance_mahalanbois,\n",
    "#             'conf_interval_mahalanobis':conf_interval_mahalanobis,\n",
    "# #             'negative_conf_interval_mahalanobis':negative_conf_interval_mahalanobis,\n",
    "# #             'positive_conf_interval_mahalanobis':positive_conf_interval_mahalanobis,\n",
    "# #             'conf_interval':conf_interval,\n",
    "# #             'negative_conf_interval':negative_conf_interval,\n",
    "# #             'positive_conf_interval':positive_conf_interval,\n",
    "#             'positive_samples':len(positive_indecies),\n",
    "#             'negative_samples':len(negative_indecies)\n",
    "#         }\n",
    "#         cluster_to_cluster[compare_cluster] = d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # activity_regularizer=tf.keras.regularizers.L1L2(l1=0.000001,l2=0.000001)\n",
    "# def create_body(image_shape):\n",
    "#     inputs = tf.keras.layers.Input(shape=image_shape)\n",
    "    \n",
    "#     def resid_block(layer_inputs, n_filters, kernel_size, strides=1):\n",
    "#         x = tf.keras.layers.Conv2D(n_filters, kernel_size, padding=\"same\", strides=strides)(layer_inputs)\n",
    "#         x = tf.keras.layers.BatchNormalization()(x)\n",
    "#         x = tf.keras.layers.ReLU()(x)\n",
    "#         x = tf.keras.layers.Conv2D(n_filters, kernel_size, padding=\"same\")(x)\n",
    "#         x = tf.keras.layers.BatchNormalization()(x)\n",
    "#         if(strides != 1 or n_filters != layer_inputs.shape[-1]):\n",
    "#             layer_inputs = tf.keras.layers.Conv2D(n_filters, (1,1), padding=\"same\", strides=strides)(layer_inputs)\n",
    "#             layer_inputs = tf.keras.layers.BatchNormalization()(layer_inputs)\n",
    "#         x = x + layer_inputs\n",
    "#         x = tf.keras.layers.ReLU()(x)\n",
    "#         return x\n",
    "    \n",
    "#     x = resid_block(inputs, config[\"FIRST_FILTERS\"], config[\"FIRST_KERNEL_SIZE\"], strides=1)\n",
    "#     for _ in range(config[\"CONV_LAYERS\"]):\n",
    "#         x = resid_block(x, config[\"N_FILTERS\"], config[\"KERNEL_SIZE\"])\n",
    "    \n",
    "#     x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "# #     x = tf.keras.layers.Dense(config[\"DENSE_SIZE\"])(x)\n",
    "# #     x = tf.keras.layers.BatchNormalization()(x)\n",
    "# #     x = tf.keras.layers.ReLU()(x)\n",
    "#     x = tf.keras.layers.Dropout(config[\"DROPOUT\"])(x)\n",
    "#     outputs = tf.keras.layers.Dense(config[\"EMBEDDING_SIZE\"])(x)\n",
    "#     return tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"body\")\n",
    "\n",
    "# body = create_body(X_train.shape[1:])\n",
    "# body.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
