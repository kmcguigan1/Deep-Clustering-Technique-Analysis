{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c715102c-fe4e-4ca7-a12e-3e3d7c4a1836",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb99a197-0acb-4868-8889-414f01783ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necesary packages\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c4a48d-efdb-4d9e-8a4c-e48380aee714",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Basic information\n",
    "    \"AUTHOR\": \"Kiernan\",\n",
    "    \n",
    "    # Data information\n",
    "    \"IMAGE_SIZE\": 28,\n",
    "    \n",
    "    # Training params\n",
    "    \"LR_STYLE\": \"REDUCE\", #['REDUCE', 'SCHEDULE']\n",
    "    \"LR\": 0.001, #0.000001,\n",
    "    \"BATCH_SIZE\": 50,\n",
    "    \"EPOCHS\": 30,\n",
    "    \n",
    "    # Loss parameters\n",
    "    \"LOSS\": \"SEMI_HARD\", #['HARD', 'SEMI_HARD']\n",
    "    \"MARGIN\": 0.5,\n",
    "    \"SOFT\": False,\n",
    "    \"DIST_METRIC\": \"L2\", #['L2','squared-L2','angular']\n",
    "    \n",
    "    # Model params\n",
    "    \"RUN_FOR_BASE\": \"3cmjv1lo\",\n",
    "    \"FREEZE\": \"ALL\", #['ALL', 'BN', 'None'] which layers to freeze in the body model\n",
    "    \"EMBEDDING_SIZE\": 16,\n",
    "    \"DROPOUT\": 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d801b08-d2f6-4223-9a1e-e667c751a02a",
   "metadata": {},
   "source": [
    "## **Initialize WANDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0872690-27c7-441b-8dc2-91073750b10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mall-off-nothing\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kiern/.netrc\n",
      "C:\\Users\\kiern\\anaconda3\\envs\\deep-clustering-analysis\\lib\\site-packages\\IPython\\html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kmcguigan/deep-clustering-evaluation/runs/kezk8tcs\" target=\"_blank\">quiet-energy-27</a></strong> to <a href=\"https://wandb.ai/kmcguigan/deep-clustering-evaluation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from secrets import WANDB\n",
    "wandb.login(key=WANDB)\n",
    "run = wandb.init(project=\"deep-clustering-evaluation\", entity=\"kmcguigan\", group=\"triplet-model\", config=config, job_type=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e10c4d9-37d2-4b86-b67a-e5ee9d9462bf",
   "metadata": {},
   "source": [
    "## **Loading Data**\n",
    "\n",
    "### **Load the presplit data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a66ea7-7acf-4679-a38f-8d5e755544f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (50000, 28, 28, 1) Val data shape: (10000, 28, 28, 1) Test data shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "with open('data/train.npy', mode='rb') as infile:\n",
    "    X_train = np.load(infile, allow_pickle=True)\n",
    "    y_train = np.load(infile, allow_pickle=True)\n",
    "\n",
    "with open('data/val.npy', mode='rb') as infile:\n",
    "    X_val = np.load(infile, allow_pickle=True)\n",
    "    y_val = np.load(infile, allow_pickle=True)\n",
    "\n",
    "with open('data/test.npy', mode='rb') as infile:\n",
    "    X_test = np.load(infile, allow_pickle=True)\n",
    "    y_test = np.load(infile, allow_pickle=True)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape} Val data shape: {X_val.shape} Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf3f8e-b48d-44c3-9958-d66adaf19bd2",
   "metadata": {},
   "source": [
    "### **Create a data generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b37c81-98bc-4ec3-883a-c1e9f0829c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, classes):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # image meta\n",
    "        self.dims = (config[\"IMAGE_SIZE\"], config[\"IMAGE_SIZE\"])\n",
    "        self.channels = 1\n",
    "        \n",
    "        # save the meta on what we will be choosing\n",
    "        self.batch_size = config[\"BATCH_SIZE\"]\n",
    "        self.samples_per_class = self.batch_size // len(classes)\n",
    "        assert(self.batch_size % self.samples_per_class == 0)\n",
    "        \n",
    "        # create the image loader\n",
    "        self.indexer = {}\n",
    "        # min_samples = None\n",
    "        for cls in classes:\n",
    "            self.indexer[cls] = np.where(y==cls)[0]\n",
    "            \n",
    "        # save the size of a single epoch of data\n",
    "        self.batches_per_epoch = (self.X.shape[0] // self.batch_size) + 1\n",
    "        \n",
    "        super(CustomDataset, self).__init__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.batches_per_epoch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        gc.collect()\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = np.empty((self.batch_size, *self.dims, self.channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        batch_idx = 0\n",
    "        for cls in self.indexer.keys():\n",
    "            samples = np.random.choice(self.indexer[cls], size=self.samples_per_class, replace=False)\n",
    "            X[batch_idx:batch_idx+self.samples_per_class, :, :, :] = self.X[samples, :, :, :]\n",
    "            y[batch_idx:batch_idx+self.samples_per_class] = self.y[samples]\n",
    "            batch_idx = batch_idx+self.samples_per_class\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9deb101-318b-40b9-a4a1-a7367afb639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CustomDataset(X_train, y_train, [i for i in range(10)])\n",
    "val_ds = CustomDataset(X_val, y_val, [i for i in range(10)])\n",
    "test_ds = CustomDataset(X_test, y_test, [i for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee27cc-8309-4bc8-b674-14f92f4f1aa7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Define Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f56a08-4d69-4b83-ae15-f0191a94e6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pairwise_distance(embeddings, squared=False):\n",
    "    dot = tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "    square_norm = tf.linalg.diag_part(dot)\n",
    "    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot + tf.expand_dims(square_norm, 0)\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "    if(not squared):\n",
    "        mask = tf.cast(tf.equal(distances, 0.0), tf.float32)\n",
    "        distances = distances + mask * 1e-16\n",
    "        distances = tf.sqrt(distances)\n",
    "        distances = distances * (1.0 - mask)\n",
    "    return distances\n",
    "\n",
    "def angular_distances(embeddings):\n",
    "    embeddings = tf.math.l2_normalize(embeddings, axis=-1)\n",
    "    angular_distances = 1 - tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "    angular_distances = tf.maximum(angular_distances, 0.0)\n",
    "    mask_offdiag = tf.ones_like(angular_distances) - tf.linalg.diag(tf.ones([tf.shape(angular_distances)[0]]))\n",
    "    angular_distances = tf.math.multiply(angular_distances, mask_offdiag)\n",
    "    return angular_distances\n",
    "\n",
    "def apply_metric(embeddings, labels, metric):\n",
    "    adj = tf.equal(labels, tf.transpose(labels))\n",
    "    adj_not = tf.math.logical_not(adj)\n",
    "    adj = tf.cast(adj, tf.float32) - tf.linalg.diag(tf.ones([tf.shape(labels)[0]]))\n",
    "    adj_not = tf.cast(adj_not, tf.float32)\n",
    "    distances = metric(embeddings)\n",
    "    pos_dist = tf.math.multiply(distances, adj)\n",
    "    neg_dist = tf.math.multiply(distances, adj_not)\n",
    "    pos_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(pos_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    neg_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(neg_dist, mask=tf.math.equal(adj_not, 1.0)))\n",
    "    return pos_dist_mean, neg_dist_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e557a990-912b-40fb-973f-563a6b63c08e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def positive_distance(labels, embeddings):\n",
    "    adj = tf.equal(labels, tf.transpose(labels))\n",
    "    adj = tf.cast(adj, tf.float32) - tf.linalg.diag(tf.ones([tf.shape(labels)[0]]))\n",
    "    distances = pairwise_distance(embeddings)\n",
    "    pos_dist = tf.math.multiply(distances, adj)\n",
    "    pos_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(pos_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    return pos_dist_mean\n",
    "\n",
    "def negative_distance(labels, embeddings):\n",
    "    adj = tf.math.logical_not(tf.equal(labels, tf.transpose(labels)))\n",
    "    adj = tf.cast(adj, tf.float32)\n",
    "    distances = pairwise_distance(embeddings)\n",
    "    neg_dist = tf.math.multiply(distances, adj)\n",
    "    neg_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(neg_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    return neg_dist_mean\n",
    "\n",
    "def positive_angular(labels, embeddings):\n",
    "    adj = tf.equal(labels, tf.transpose(labels))\n",
    "    adj = tf.cast(adj, tf.float32) - tf.linalg.diag(tf.ones([tf.shape(labels)[0]]))\n",
    "    distances = angular_distances(embeddings)\n",
    "    pos_dist = tf.math.multiply(distances, adj)\n",
    "    pos_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(pos_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    return pos_dist_mean\n",
    "\n",
    "def negative_angular(labels, embeddings):\n",
    "    adj = tf.math.logical_not(tf.equal(labels, tf.transpose(labels)))\n",
    "    adj = tf.cast(adj, tf.float32)\n",
    "    distances = angular_distances(embeddings)\n",
    "    neg_dist = tf.math.multiply(distances, adj)\n",
    "    neg_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(neg_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    return neg_dist_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c71bb22-4d92-48f5-a464-5e4a662ba2fa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetricHandler:\n",
    "    def __init__(self, metric):\n",
    "        self.has_read = {0: False, 1: False}\n",
    "        self.metric = metric\n",
    "        self.results = {0: None, 1: None}\n",
    "        \n",
    "    def read_metric(self, reader, embeddings, labels):\n",
    "        if(self.has_read[reader]):\n",
    "            raise Excpetion(f'{reader} reader re-reading data it already has')\n",
    "        other = 1 - reader\n",
    "        if(self.has_read[other]):\n",
    "            value = self.results[reader]\n",
    "            self.results[0] = None\n",
    "            self.results[1] = None\n",
    "            return value\n",
    "        metric_results = apply_metric(embeddings, labels, self.metric)\n",
    "        self.results[0] = metric_results[0]\n",
    "        self.results[1] = metric_results[1]\n",
    "        return self.results[reader]\n",
    "    \n",
    "distance_handler = MetricHandler(pairwise_distance)\n",
    "angular_handler = MetricHandler(angular_distances)\n",
    "    \n",
    "def pos_distance(labels, embeddings):\n",
    "    return distance_handler.read_metric(0, embeddings, labels)\n",
    "def neg_distance(labels, embeddings):\n",
    "    return distance_handler.read_metric(1, embeddings, labels)\n",
    "\n",
    "def pos_angle(labels, embeddings):\n",
    "    return angular_handler.read_metric(0, embeddings, labels)\n",
    "def neg_angle(labels, embeddings):\n",
    "    return angular_handler.read_metric(1, embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ee275f-ff75-455a-9371-31922ebd1fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lr_callback(plot=False, batch_size=config['BATCH_SIZE'], epochs=config['EPOCHS']):\n",
    "    lr_start   = config['LR']\n",
    "    lr_max     = config['LR'] * 5 * batch_size  \n",
    "    lr_min     = config['LR']\n",
    "    lr_ramp_ep = 4\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.9\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        return lr\n",
    "    if(plot):\n",
    "        epochs = list(range(epochs))\n",
    "        learning_rates = [lrfn(x) for x in epochs]\n",
    "        plt.scatter(epochs,learning_rates)\n",
    "        ax = plt.gca()\n",
    "        ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "        plt.show()\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback\n",
    "\n",
    "if(config[\"LR_STYLE\"] == \"SCHEDULE\"):\n",
    "    lr_callback = get_lr_callback(plot=True)\n",
    "elif(config[\"LR_STYLE\"] == \"REDUCE\"):\n",
    "    lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=2)\n",
    "else:\n",
    "    raise Exception(f\"config LR_STYLE {config['LR_STYLE']} is not understood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8215aa-fd5c-4385-ab19-0e42b809feee",
   "metadata": {},
   "source": [
    "## **Create Model**\n",
    "\n",
    "### **Load the pretrained body model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "827c5c3f-5523-49a9-96dc-ed4277684b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "def freeze_BN(model):\n",
    "    # Unfreeze layers while leaving BatchNorm layers frozen\n",
    "    for layer in model.layers:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "            \n",
    "def freeze_none(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae2343e8-f9bf-48d0-b1d3-bb36233004a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"body\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 14, 16)        160       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 14, 14, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 8)         1160      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 14, 14, 16)        144       \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 16)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,408\n",
      "Trainable params: 0\n",
      "Non-trainable params: 3,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "body_file = wandb.restore('body.h5', run_path=f\"kmcguigan/deep-clustering-evaluation/{config['RUN_FOR_BASE']}\")\n",
    "body = tf.keras.models.load_model(body_file.name)\n",
    "body_output_shape = body.layers[-1].output_shape[-1]\n",
    "if(config[\"FREEZE\"] == \"ALL\"):\n",
    "    freeze_all(body)\n",
    "elif(config[\"FREEZE\"] == \"BN\"):\n",
    "    freeze_BN(body)\n",
    "elif(config[\"FREEZE\"] == \"None\"):\n",
    "    freeze_none(body)\n",
    "else:\n",
    "    raise Excpetion(f\"config FREEZE is set to {config['FREEZE']} but this freeze method is not understood\")\n",
    "body.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8d342-c250-4868-bad2-b0544c6edb17",
   "metadata": {},
   "source": [
    "### **Create the head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3540639c-6bd9-4fca-af1e-db7983acdb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 16)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 272\n",
      "Trainable params: 272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_head(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=(input_shape,))\n",
    "    x = tf.keras.layers.Dropout(config[\"DROPOUT\"])(inputs)\n",
    "    x = tf.keras.layers.Dense(config['EMBEDDING_SIZE'])(x)\n",
    "    outputs = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1))(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"head\")\n",
    "\n",
    "head = create_head(input_shape=body_output_shape)\n",
    "head.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04524c96-75a1-4586-b4a9-91af25c6fde0",
   "metadata": {},
   "source": [
    "### **Create the full model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fe1b374-3ff4-483b-9e14-5396b2734c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"combinedModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " body (Functional)           (None, 16)                3408      \n",
      "                                                                 \n",
      " head (Functional)           (None, 16)                272       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,680\n",
      "Trainable params: 272\n",
      "Non-trainable params: 3,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    body,\n",
    "    head\n",
    "],\n",
    "    name=\"combinedModel\"\n",
    ")\n",
    "if(config[\"LOSS\"] == \"HARD\"):\n",
    "    loss = tfa.losses.TripletHardLoss(margin=config[\"MARGIN\"], distance_metric=config[\"DIST_METRIC\"], soft=config[\"SOFT\"])\n",
    "elif(config[\"LOSS\"] == \"SEMI_HARD\"):\n",
    "    loss = tfa.losses.TripletSemiHardLoss(margin=config[\"MARGIN\"], distance_metric=config[\"DIST_METRIC\"], soft=config[\"SOFT\"])\n",
    "else:\n",
    "    raise Exception(f\"config LOSS of {config['LOSS']} is not understood\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=config['LR'])\n",
    "\n",
    "metrics = [\n",
    "    positive_distance,\n",
    "    negative_distance,\n",
    "    positive_angular,\n",
    "    negative_angular\n",
    "]\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7fecc7-7b8b-44c9-b161-e4696e393f55",
   "metadata": {},
   "source": [
    "## **Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c993214d-cbcc-4505-882b-b180355fab6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1001/1001 [==============================] - 12s 9ms/step - loss: 0.2282 - positive_distance: 0.6086 - negative_distance: 1.4398 - positive_angular: 0.2097 - negative_angular: 1.0736 - val_loss: 0.1263 - val_positive_distance: 0.4431 - val_negative_distance: 1.4495 - val_positive_angular: 0.1276 - val_negative_angular: 1.0887 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1970 - positive_distance: 0.5460 - negative_distance: 1.4466 - positive_angular: 0.1744 - negative_angular: 1.0858 - val_loss: 0.1156 - val_positive_distance: 0.4335 - val_negative_distance: 1.4522 - val_positive_angular: 0.1225 - val_negative_angular: 1.0920 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1959 - positive_distance: 0.5389 - negative_distance: 1.4464 - positive_angular: 0.1718 - negative_angular: 1.0860 - val_loss: 0.1126 - val_positive_distance: 0.4285 - val_negative_distance: 1.4517 - val_positive_angular: 0.1191 - val_negative_angular: 1.0909 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1940 - positive_distance: 0.5356 - negative_distance: 1.4464 - positive_angular: 0.1702 - negative_angular: 1.0860 - val_loss: 0.1234 - val_positive_distance: 0.4399 - val_negative_distance: 1.4505 - val_positive_angular: 0.1262 - val_negative_angular: 1.0901 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1944 - positive_distance: 0.5351 - negative_distance: 1.4466 - positive_angular: 0.1694 - negative_angular: 1.0864 - val_loss: 0.1209 - val_positive_distance: 0.4384 - val_negative_distance: 1.4520 - val_positive_angular: 0.1246 - val_negative_angular: 1.0920 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1935 - positive_distance: 0.5338 - negative_distance: 1.4468 - positive_angular: 0.1688 - negative_angular: 1.0867 - val_loss: 0.1187 - val_positive_distance: 0.4340 - val_negative_distance: 1.4517 - val_positive_angular: 0.1220 - val_negative_angular: 1.0916 - lr: 9.0000e-04\n",
      "Epoch 7/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1949 - positive_distance: 0.5339 - negative_distance: 1.4470 - positive_angular: 0.1689 - negative_angular: 1.0870 - val_loss: 0.1171 - val_positive_distance: 0.4368 - val_negative_distance: 1.4522 - val_positive_angular: 0.1236 - val_negative_angular: 1.0917 - lr: 9.0000e-04\n"
     ]
    }
   ],
   "source": [
    "stopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "hist = model.fit(train_ds,\n",
    "                 validation_data=val_ds,\n",
    "                 epochs=config[\"EPOCHS\"],\n",
    "                 callbacks=[stopper, lr_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b437b75-cea9-442a-a47c-76f107f6bf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/201 [==============================] - 1s 6ms/step - loss: 0.1092 - positive_distance: 0.4254 - negative_distance: 1.4525 - positive_angular: 0.1165 - negative_angular: 1.0917\n"
     ]
    }
   ],
   "source": [
    "ev = model.evaluate(test_ds, return_dict=True)\n",
    "log_dict = {f'test/{met}': val for met, val in ev.items()}\n",
    "run.log(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04d7c49a-bbf9-4296-9104-7f59d1f57afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 6, 1: 2, 2: 3, 3: 9, 4: 7, 5: 1, 6: 0, 7: 8, 8: 4, 9: 5}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kmeans_cluster_accuracy(X, y):\n",
    "    embeddings = model.predict(X)\n",
    "    kmeans = KMeans(n_clusters=10, random_state=123)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    label_mappings = {}\n",
    "    for label in np.unique(labels):\n",
    "        values, counts = np.unique(y_test[np.where(labels==label)], return_counts=True)\n",
    "        label_mappings[label] = values[np.argmax(counts)]\n",
    "    print(label_mappings)\n",
    "    \n",
    "    map_labels = np.vectorize(lambda x: label_mappings[x])\n",
    "    mapped_labels = map_labels(labels)\n",
    "    return tf.keras.metrics.categorical_accuracy(y_test, mapped_labels).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9383e-b579-473f-988e-3e61d2916dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = kmeans_cluster_accuracy(X_test, y_test)\n",
    "run.log({'test/test-clustering-accuracy': acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017cc4d4-7ac2-40d9-9bc8-782b398c176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = kmeans_cluster_accuracy(X_val, y_val)\n",
    "run.log({'test/val-clustering-accuracy': acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b1c4a49-f397-4c4d-aebc-be6ae7554f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12884... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>loss</td><td>█▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████▁▁</td></tr><tr><td>negative_angular</td><td>▁▇▇▇███</td></tr><tr><td>negative_distance</td><td>▁█▇▇███</td></tr><tr><td>positive_angular</td><td>█▂▂▁▁▁▁</td></tr><tr><td>positive_distance</td><td>█▂▁▁▁▁▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/negative_angular</td><td>▁</td></tr><tr><td>test/negative_distance</td><td>▁</td></tr><tr><td>test/positive_angular</td><td>▁</td></tr><tr><td>test/positive_distance</td><td>▁</td></tr><tr><td>val_loss</td><td>█▃▁▆▅▄▃</td></tr><tr><td>val_negative_angular</td><td>▁█▆▄█▇▇</td></tr><tr><td>val_negative_distance</td><td>▁█▇▄▇▇█</td></tr><tr><td>val_positive_angular</td><td>█▄▁▇▆▃▅</td></tr><tr><td>val_positive_distance</td><td>█▃▁▆▆▄▅</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.11261</td></tr><tr><td>epoch</td><td>6</td></tr><tr><td>loss</td><td>0.19494</td></tr><tr><td>lr</td><td>0.0009</td></tr><tr><td>negative_angular</td><td>1.08702</td></tr><tr><td>negative_distance</td><td>1.44699</td></tr><tr><td>positive_angular</td><td>0.16888</td></tr><tr><td>positive_distance</td><td>0.5339</td></tr><tr><td>test/loss</td><td>0.10919</td></tr><tr><td>test/negative_angular</td><td>1.0917</td></tr><tr><td>test/negative_distance</td><td>1.45251</td></tr><tr><td>test/positive_angular</td><td>0.11647</td></tr><tr><td>test/positive_distance</td><td>0.42543</td></tr><tr><td>val_loss</td><td>0.11706</td></tr><tr><td>val_negative_angular</td><td>1.09165</td></tr><tr><td>val_negative_distance</td><td>1.45223</td></tr><tr><td>val_positive_angular</td><td>0.12365</td></tr><tr><td>val_positive_distance</td><td>0.43681</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 2 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">quiet-energy-27</strong>: <a href=\"https://wandb.ai/kmcguigan/deep-clustering-evaluation/runs/kezk8tcs\" target=\"_blank\">https://wandb.ai/kmcguigan/deep-clustering-evaluation/runs/kezk8tcs</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220306_202312-kezk8tcs\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad443da1-b31b-401c-a470-94dcf1debb94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8758cf68-5e1c-42e3-88c7-42c6ff4ea201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
