{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c715102c-fe4e-4ca7-a12e-3e3d7c4a1836",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb99a197-0acb-4868-8889-414f01783ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necesary packages\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c4a48d-efdb-4d9e-8a4c-e48380aee714",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Basic information\n",
    "    \"AUTHOR\": \"Kiernan\",\n",
    "    \n",
    "    # Data information\n",
    "    \"IMAGE_SIZE\": 28,\n",
    "    \n",
    "    # Training params\n",
    "    \"LR_STYLE\": \"REDUCE\", #['REDUCE', 'SCHEDULE']\n",
    "    \"LR\": 0.001, #0.000001,\n",
    "    \"BATCH_SIZE\": 50,\n",
    "    \"EPOCHS\": 30,\n",
    "    \n",
    "    # Loss parameters\n",
    "    \"LOSS\": \"SEMI_HARD\", #['HARD', 'SEMI_HARD']\n",
    "    \"MARGIN\": 0.5,\n",
    "    \"SOFT\": False,\n",
    "    \"DIST_METRIC\": \"L2\", #['L2','squared-L2','angular']\n",
    "    \n",
    "    # Model params\n",
    "    \"RUN_FOR_BASE\": \"3cmjv1lo\",\n",
    "    \"FREEZE\": \"ALL\", #['ALL', 'BN', 'None'] which layers to freeze in the body model\n",
    "    \"EMBEDDING_SIZE\": 16,\n",
    "    \"DROPOUT\": 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d801b08-d2f6-4223-9a1e-e667c751a02a",
   "metadata": {},
   "source": [
    "## **Initialize WANDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0872690-27c7-441b-8dc2-91073750b10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mall-off-nothing\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kiern/.netrc\n",
      "C:\\Users\\kiern\\anaconda3\\envs\\deep-clustering-analysis\\lib\\site-packages\\IPython\\html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kmcguigan/deep-clustering-evaluation/runs/3liueeqj\" target=\"_blank\">floral-elevator-26</a></strong> to <a href=\"https://wandb.ai/kmcguigan/deep-clustering-evaluation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from secrets import WANDB\n",
    "wandb.login(key=WANDB)\n",
    "run = wandb.init(project=\"deep-clustering-evaluation\", entity=\"kmcguigan\", group=\"triplet-model\", config=config, job_type=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e10c4d9-37d2-4b86-b67a-e5ee9d9462bf",
   "metadata": {},
   "source": [
    "## **Loading Data**\n",
    "\n",
    "### **Load the presplit data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a66ea7-7acf-4679-a38f-8d5e755544f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (50000, 28, 28, 1) Val data shape: (10000, 28, 28, 1) Test data shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "with open('data/train.npy', mode='rb') as infile:\n",
    "    X_train = np.load(infile, allow_pickle=True)\n",
    "    y_train = np.load(infile, allow_pickle=True)\n",
    "\n",
    "with open('data/val.npy', mode='rb') as infile:\n",
    "    X_val = np.load(infile, allow_pickle=True)\n",
    "    y_val = np.load(infile, allow_pickle=True)\n",
    "\n",
    "with open('data/test.npy', mode='rb') as infile:\n",
    "    X_test = np.load(infile, allow_pickle=True)\n",
    "    y_test = np.load(infile, allow_pickle=True)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape} Val data shape: {X_val.shape} Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf3f8e-b48d-44c3-9958-d66adaf19bd2",
   "metadata": {},
   "source": [
    "### **Create a data generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b37c81-98bc-4ec3-883a-c1e9f0829c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, classes):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # image meta\n",
    "        self.dims = (config[\"IMAGE_SIZE\"], config[\"IMAGE_SIZE\"])\n",
    "        self.channels = 1\n",
    "        \n",
    "        # save the meta on what we will be choosing\n",
    "        self.batch_size = config[\"BATCH_SIZE\"]\n",
    "        self.samples_per_class = self.batch_size // len(classes)\n",
    "        assert(self.batch_size % self.samples_per_class == 0)\n",
    "        \n",
    "        # create the image loader\n",
    "        self.indexer = {}\n",
    "        # min_samples = None\n",
    "        for cls in classes:\n",
    "            self.indexer[cls] = np.where(y==cls)[0]\n",
    "            \n",
    "        # save the size of a single epoch of data\n",
    "        self.batches_per_epoch = (self.X.shape[0] // self.batch_size) + 1\n",
    "        \n",
    "        super(CustomDataset, self).__init__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.batches_per_epoch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        gc.collect()\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = np.empty((self.batch_size, *self.dims, self.channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        batch_idx = 0\n",
    "        for cls in self.indexer.keys():\n",
    "            samples = np.random.choice(self.indexer[cls], size=self.samples_per_class, replace=False)\n",
    "            X[batch_idx:batch_idx+self.samples_per_class, :, :, :] = self.X[samples, :, :, :]\n",
    "            y[batch_idx:batch_idx+self.samples_per_class] = self.y[samples]\n",
    "            batch_idx = batch_idx+self.samples_per_class\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9deb101-318b-40b9-a4a1-a7367afb639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CustomDataset(X_train, y_train, [i for i in range(10)])\n",
    "val_ds = CustomDataset(X_val, y_val, [i for i in range(10)])\n",
    "test_ds = CustomDataset(X_test, y_test, [i for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee27cc-8309-4bc8-b674-14f92f4f1aa7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Define Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f56a08-4d69-4b83-ae15-f0191a94e6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pairwise_distance(embeddings, squared=False):\n",
    "    dot = tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "    square_norm = tf.linalg.diag_part(dot)\n",
    "    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot + tf.expand_dims(square_norm, 0)\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "    if(not squared):\n",
    "        mask = tf.cast(tf.equal(distances, 0.0), tf.float32)\n",
    "        distances = distances + mask * 1e-16\n",
    "        distances = tf.sqrt(distances)\n",
    "        distances = distances * (1.0 - mask)\n",
    "    return distances\n",
    "\n",
    "def angular_distances(embeddings):\n",
    "    embeddings = tf.math.l2_normalize(embeddings, axis=-1)\n",
    "    angular_distances = 1 - tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "    angular_distances = tf.maximum(angular_distances, 0.0)\n",
    "    mask_offdiag = tf.ones_like(angular_distances) - tf.linalg.diag(tf.ones([tf.shape(angular_distances)[0]]))\n",
    "    angular_distances = tf.math.multiply(angular_distances, mask_offdiag)\n",
    "    return angular_distances\n",
    "\n",
    "def apply_metric(embeddings, labels, metric):\n",
    "    adj = tf.equal(labels, tf.transpose(labels))\n",
    "    adj_not = tf.math.logical_not(adj)\n",
    "    adj = tf.cast(adj, tf.float32) - tf.linalg.diag(tf.ones([tf.shape(labels)[0]]))\n",
    "    adj_not = tf.cast(adj_not, tf.float32)\n",
    "    distances = metric(embeddings)\n",
    "    pos_dist = tf.math.multiply(distances, adj)\n",
    "    neg_dist = tf.math.multiply(distances, adj_not)\n",
    "    pos_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(pos_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    neg_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(neg_dist, mask=tf.math.equal(adj_not, 1.0)))\n",
    "    return pos_dist_mean, neg_dist_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e557a990-912b-40fb-973f-563a6b63c08e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def positive_distance(labels, embeddings):\n",
    "    adj = tf.equal(labels, tf.transpose(labels))\n",
    "    adj = tf.cast(adj, tf.float32) - tf.linalg.diag(tf.ones([tf.shape(labels)[0]]))\n",
    "    distances = pairwise_distance(embeddings)\n",
    "    pos_dist = tf.math.multiply(distances, adj)\n",
    "    pos_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(pos_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    return pos_dist_mean\n",
    "\n",
    "def negative_distance(labels, embeddings):\n",
    "    adj = tf.math.logical_not(tf.equal(labels, tf.transpose(labels)))\n",
    "    adj = tf.cast(adj, tf.float32)\n",
    "    distances = pairwise_distance(embeddings)\n",
    "    neg_dist = tf.math.multiply(distances, adj)\n",
    "    neg_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(neg_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    return neg_dist_mean\n",
    "\n",
    "def positive_angular(labels, embeddings):\n",
    "    adj = tf.equal(labels, tf.transpose(labels))\n",
    "    adj = tf.cast(adj, tf.float32) - tf.linalg.diag(tf.ones([tf.shape(labels)[0]]))\n",
    "    distances = angular_distances(embeddings)\n",
    "    pos_dist = tf.math.multiply(distances, adj)\n",
    "    pos_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(pos_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    return pos_dist_mean\n",
    "\n",
    "def negative_angular(labels, embeddings):\n",
    "    adj = tf.math.logical_not(tf.equal(labels, tf.transpose(labels)))\n",
    "    adj = tf.cast(adj, tf.float32)\n",
    "    distances = angular_distances(embeddings)\n",
    "    neg_dist = tf.math.multiply(distances, adj)\n",
    "    neg_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(neg_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    return neg_dist_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c71bb22-4d92-48f5-a464-5e4a662ba2fa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetricHandler:\n",
    "    def __init__(self, metric):\n",
    "        self.has_read = {0: False, 1: False}\n",
    "        self.metric = metric\n",
    "        self.results = {0: None, 1: None}\n",
    "        \n",
    "    def read_metric(self, reader, embeddings, labels):\n",
    "        if(self.has_read[reader]):\n",
    "            raise Excpetion(f'{reader} reader re-reading data it already has')\n",
    "        other = 1 - reader\n",
    "        if(self.has_read[other]):\n",
    "            value = self.results[reader]\n",
    "            self.results[0] = None\n",
    "            self.results[1] = None\n",
    "            return value\n",
    "        metric_results = apply_metric(embeddings, labels, self.metric)\n",
    "        self.results[0] = metric_results[0]\n",
    "        self.results[1] = metric_results[1]\n",
    "        return self.results[reader]\n",
    "    \n",
    "distance_handler = MetricHandler(pairwise_distance)\n",
    "angular_handler = MetricHandler(angular_distances)\n",
    "    \n",
    "def pos_distance(labels, embeddings):\n",
    "    return distance_handler.read_metric(0, embeddings, labels)\n",
    "def neg_distance(labels, embeddings):\n",
    "    return distance_handler.read_metric(1, embeddings, labels)\n",
    "\n",
    "def pos_angle(labels, embeddings):\n",
    "    return angular_handler.read_metric(0, embeddings, labels)\n",
    "def neg_angle(labels, embeddings):\n",
    "    return angular_handler.read_metric(1, embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ee275f-ff75-455a-9371-31922ebd1fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lr_callback(plot=False, batch_size=config['BATCH_SIZE'], epochs=config['EPOCHS']):\n",
    "    lr_start   = config['LR']\n",
    "    lr_max     = config['LR'] * 5 * batch_size  \n",
    "    lr_min     = config['LR']\n",
    "    lr_ramp_ep = 4\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.9\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        return lr\n",
    "    if(plot):\n",
    "        epochs = list(range(epochs))\n",
    "        learning_rates = [lrfn(x) for x in epochs]\n",
    "        plt.scatter(epochs,learning_rates)\n",
    "        ax = plt.gca()\n",
    "        ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "        plt.show()\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback\n",
    "\n",
    "if(config[\"LR_STYLE\"] == \"SCHEDULE\"):\n",
    "    lr_callback = get_lr_callback(plot=True)\n",
    "elif(config[\"LR_STYLE\"] == \"REDUCE\"):\n",
    "    lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=2)\n",
    "else:\n",
    "    raise Exception(f\"config LR_STYLE {config['LR_STYLE']} is not understood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8215aa-fd5c-4385-ab19-0e42b809feee",
   "metadata": {},
   "source": [
    "## **Create Model**\n",
    "\n",
    "### **Load the pretrained body model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "827c5c3f-5523-49a9-96dc-ed4277684b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "def freeze_BN(model):\n",
    "    # Unfreeze layers while leaving BatchNorm layers frozen\n",
    "    for layer in model.layers:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "            \n",
    "def freeze_none(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae2343e8-f9bf-48d0-b1d3-bb36233004a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"body\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 14, 16)        160       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 14, 14, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 8)         1160      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 14, 14, 16)        144       \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 16)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,408\n",
      "Trainable params: 0\n",
      "Non-trainable params: 3,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "body_file = wandb.restore('body.h5', run_path=f\"kmcguigan/deep-clustering-evaluation/{config['RUN_FOR_BASE']}\")\n",
    "body = tf.keras.models.load_model(body_file.name)\n",
    "body_output_shape = body.layers[-1].output_shape[-1]\n",
    "if(config[\"FREEZE\"] == \"ALL\"):\n",
    "    freeze_all(body)\n",
    "elif(config[\"FREEZE\"] == \"BN\"):\n",
    "    freeze_BN(body)\n",
    "elif(config[\"FREEZE\"] == \"None\"):\n",
    "    freeze_none(body)\n",
    "else:\n",
    "    raise Excpetion(f\"config FREEZE is set to {config['FREEZE']} but this freeze method is not understood\")\n",
    "body.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8d342-c250-4868-bad2-b0544c6edb17",
   "metadata": {},
   "source": [
    "### **Create the head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3540639c-6bd9-4fca-af1e-db7983acdb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 16)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 272\n",
      "Trainable params: 272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_head(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=(input_shape,))\n",
    "    x = tf.keras.layers.Dropout(config[\"DROPOUT\"])(inputs)\n",
    "    x = tf.keras.layers.Dense(config['EMBEDDING_SIZE'])(x)\n",
    "    outputs = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1))(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"head\")\n",
    "\n",
    "head = create_head(input_shape=body_output_shape)\n",
    "head.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04524c96-75a1-4586-b4a9-91af25c6fde0",
   "metadata": {},
   "source": [
    "### **Create the full model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fe1b374-3ff4-483b-9e14-5396b2734c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"combinedModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " body (Functional)           (None, 16)                3408      \n",
      "                                                                 \n",
      " head (Functional)           (None, 16)                272       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,680\n",
      "Trainable params: 272\n",
      "Non-trainable params: 3,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    body,\n",
    "    head\n",
    "],\n",
    "    name=\"combinedModel\"\n",
    ")\n",
    "if(config[\"LOSS\"] == \"HARD\"):\n",
    "    loss = tfa.losses.TripletHardLoss(margin=config[\"MARGIN\"], distance_metric=config[\"DIST_METRIC\"], soft=config[\"SOFT\"])\n",
    "elif(config[\"LOSS\"] == \"SEMI_HARD\"):\n",
    "    loss = tfa.losses.TripletSemiHardLoss(margin=config[\"MARGIN\"], distance_metric=config[\"DIST_METRIC\"], soft=config[\"SOFT\"])\n",
    "else:\n",
    "    raise Exception(f\"config LOSS of {config['LOSS']} is not understood\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=config['LR'])\n",
    "\n",
    "metrics = [\n",
    "    positive_distance,\n",
    "    negative_distance,\n",
    "    positive_angular,\n",
    "    negative_angular\n",
    "]\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7fecc7-7b8b-44c9-b161-e4696e393f55",
   "metadata": {},
   "source": [
    "## **Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c993214d-cbcc-4505-882b-b180355fab6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1001/1001 [==============================] - 12s 9ms/step - loss: 0.2308 - positive_distance: 0.6086 - negative_distance: 1.4392 - positive_angular: 0.2103 - negative_angular: 1.0736 - val_loss: 0.1217 - val_positive_distance: 0.4371 - val_negative_distance: 1.4511 - val_positive_angular: 0.1234 - val_negative_angular: 1.0906 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1969 - positive_distance: 0.5456 - negative_distance: 1.4471 - positive_angular: 0.1741 - negative_angular: 1.0864 - val_loss: 0.1185 - val_positive_distance: 0.4370 - val_negative_distance: 1.4517 - val_positive_angular: 0.1230 - val_negative_angular: 1.0911 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1955 - positive_distance: 0.5376 - negative_distance: 1.4466 - positive_angular: 0.1710 - negative_angular: 1.0862 - val_loss: 0.1214 - val_positive_distance: 0.4376 - val_negative_distance: 1.4510 - val_positive_angular: 0.1245 - val_negative_angular: 1.0905 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1964 - positive_distance: 0.5386 - negative_distance: 1.4467 - positive_angular: 0.1721 - negative_angular: 1.0864 - val_loss: 0.1148 - val_positive_distance: 0.4313 - val_negative_distance: 1.4516 - val_positive_angular: 0.1202 - val_negative_angular: 1.0911 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1959 - positive_distance: 0.5376 - negative_distance: 1.4467 - positive_angular: 0.1713 - negative_angular: 1.0864 - val_loss: 0.1203 - val_positive_distance: 0.4401 - val_negative_distance: 1.4512 - val_positive_angular: 0.1250 - val_negative_angular: 1.0908 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1967 - positive_distance: 0.5380 - negative_distance: 1.4470 - positive_angular: 0.1712 - negative_angular: 1.0870 - val_loss: 0.1147 - val_positive_distance: 0.4315 - val_negative_distance: 1.4516 - val_positive_angular: 0.1203 - val_negative_angular: 1.0912 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1946 - positive_distance: 0.5357 - negative_distance: 1.4470 - positive_angular: 0.1701 - negative_angular: 1.0868 - val_loss: 0.1145 - val_positive_distance: 0.4311 - val_negative_distance: 1.4514 - val_positive_angular: 0.1206 - val_negative_angular: 1.0908 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1953 - positive_distance: 0.5365 - negative_distance: 1.4471 - positive_angular: 0.1702 - negative_angular: 1.0870 - val_loss: 0.1192 - val_positive_distance: 0.4348 - val_negative_distance: 1.4521 - val_positive_angular: 0.1222 - val_negative_angular: 1.0921 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1941 - positive_distance: 0.5357 - negative_distance: 1.4473 - positive_angular: 0.1700 - negative_angular: 1.0873 - val_loss: 0.1170 - val_positive_distance: 0.4337 - val_negative_distance: 1.4520 - val_positive_angular: 0.1217 - val_negative_angular: 1.0919 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1938 - positive_distance: 0.5349 - negative_distance: 1.4472 - positive_angular: 0.1697 - negative_angular: 1.0872 - val_loss: 0.1195 - val_positive_distance: 0.4364 - val_negative_distance: 1.4513 - val_positive_angular: 0.1225 - val_negative_angular: 1.0909 - lr: 9.0000e-04\n",
      "Epoch 11/30\n",
      "1001/1001 [==============================] - 8s 8ms/step - loss: 0.1943 - positive_distance: 0.5354 - negative_distance: 1.4463 - positive_angular: 0.1695 - negative_angular: 1.0860 - val_loss: 0.1157 - val_positive_distance: 0.4325 - val_negative_distance: 1.4514 - val_positive_angular: 0.1212 - val_negative_angular: 1.0909 - lr: 9.0000e-04\n"
     ]
    }
   ],
   "source": [
    "stopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "hist = model.fit(train_ds,\n",
    "                 validation_data=val_ds,\n",
    "                 epochs=config[\"EPOCHS\"],\n",
    "                 callbacks=[stopper, lr_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b437b75-cea9-442a-a47c-76f107f6bf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/201 [==============================] - 1s 6ms/step - loss: 0.1141 - positive_distance: 0.4282 - negative_distance: 1.4524 - positive_angular: 0.1173 - negative_angular: 1.0920\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19112\\896308067.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlog_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34mf'test/{met}'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\deep-clustering-analysis\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\u001b[0m in \u001b[0;36mlog\u001b[1;34m(self, data, step, commit, sync)\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mcommit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcommit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_row_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1350\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_row_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep-clustering-analysis\\lib\\site-packages\\wandb\\sdk\\wandb_history.py\u001b[0m in \u001b[0;36m_row_add\u001b[1;34m(self, row)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_row_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep-clustering-analysis\\lib\\site-packages\\wandb\\sdk\\wandb_history.py\u001b[0m in \u001b[0;36m_flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_timestamp\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_timestamp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep-clustering-analysis\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\u001b[0m in \u001b[0;36m_history_callback\u001b[1;34m(self, row, step)\u001b[0m\n\u001b[0;32m   1026\u001b[0m             \u001b[0mnot_using_tensorboard\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tensorboard\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             self._backend.interface.publish_history(\n\u001b[1;32m-> 1028\u001b[1;33m                 \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpublish_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnot_using_tensorboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m             )\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep-clustering-analysis\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\u001b[0m in \u001b[0;36mpublish_history\u001b[1;34m(self, data, step, run, publish_step)\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_dumps_safer_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_publish_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep-clustering-analysis\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\u001b[0m in \u001b[0;36m_publish_history\u001b[1;34m(self, history)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_publish_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHistoryRecord\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mrec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_publish_preempting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreempt_rec\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunPreemptingRecord\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep-clustering-analysis\\lib\\site-packages\\wandb\\sdk\\interface\\interface_queue.py\u001b[0m in \u001b[0;36m_publish\u001b[1;34m(self, record, local)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"pb.Record\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    }
   ],
   "source": [
    "ev = model.evaluate(test_ds, return_dict=True)\n",
    "log_dict = {f'test/{met}': val for met, val in ev.items()}\n",
    "run.log(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7c49a-bbf9-4296-9104-7f59d1f57afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.predict(X_test)\n",
    "kmeans = KMeans(n_clusters=10, random_state=123)\n",
    "labels = kmeans.fit_predict(embeddings)\n",
    "labels = kmeans.predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b1c4a49-f397-4c4d-aebc-be6ae7554f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1412... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>loss</td><td>█▂▁▁▁▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▁▁</td></tr><tr><td>negative_angular</td><td>▁█▇███████▇</td></tr><tr><td>negative_distance</td><td>▁█▇▇▇█████▇</td></tr><tr><td>positive_angular</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>positive_distance</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/negative_angular</td><td>▁</td></tr><tr><td>test/negative_distance</td><td>▁</td></tr><tr><td>test/positive_angular</td><td>▁</td></tr><tr><td>test/positive_distance</td><td>▁</td></tr><tr><td>val_loss</td><td>█▅█▁▇▁▁▆▃▆▂</td></tr><tr><td>val_negative_angular</td><td>▂▄▁▃▂▄▂█▇▃▃</td></tr><tr><td>val_negative_distance</td><td>▂▅▁▅▂▅▃█▇▃▄</td></tr><tr><td>val_positive_angular</td><td>▆▅▇▁█▁▂▄▃▄▂</td></tr><tr><td>val_positive_distance</td><td>▆▆▆▁█▁▁▄▃▅▂</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.11454</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss</td><td>0.19433</td></tr><tr><td>lr</td><td>0.0009</td></tr><tr><td>negative_angular</td><td>1.08603</td></tr><tr><td>negative_distance</td><td>1.44633</td></tr><tr><td>positive_angular</td><td>0.16952</td></tr><tr><td>positive_distance</td><td>0.5354</td></tr><tr><td>test/loss</td><td>0.07677</td></tr><tr><td>test/negative_angular</td><td>1.0931</td></tr><tr><td>test/negative_distance</td><td>1.45317</td></tr><tr><td>test/positive_angular</td><td>0.09996</td></tr><tr><td>test/positive_distance</td><td>0.38611</td></tr><tr><td>val_loss</td><td>0.11573</td></tr><tr><td>val_negative_angular</td><td>1.09089</td></tr><tr><td>val_negative_distance</td><td>1.45142</td></tr><tr><td>val_positive_angular</td><td>0.12122</td></tr><tr><td>val_positive_distance</td><td>0.43255</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 2 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">floral-elevator-26</strong>: <a href=\"https://wandb.ai/kmcguigan/deep-clustering-evaluation/runs/3liueeqj\" target=\"_blank\">https://wandb.ai/kmcguigan/deep-clustering-evaluation/runs/3liueeqj</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220306_200220-3liueeqj\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad443da1-b31b-401c-a470-94dcf1debb94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8758cf68-5e1c-42e3-88c7-42c6ff4ea201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
