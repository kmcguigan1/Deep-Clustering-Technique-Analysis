{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7072d16-6d68-4d2b-81db-452669933efd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb62a108-ac41-451d-9057-4a2888f5aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necesary packages\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56eafb6-3242-41dc-8dc9-d15997e09c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Basic information\n",
    "    \"AUTHOR\": \"Kiernan\",\n",
    "    \n",
    "    # Data information\n",
    "    \"IMAGE_SIZE\": (28,28,1),\n",
    "    \n",
    "    # Training params\n",
    "    \"LR_STYLE\": \"REDUCE\", #['REDUCE', 'SCHEDULE']\n",
    "    \"LR\": 0.001, #0.000001,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"EPOCHS\": 30,\n",
    "    \n",
    "    # Loss parameters\n",
    "    \"MARGIN\": 0.5,\n",
    "    \n",
    "    # Model params\n",
    "    \"FIRST_FILTERS\": 16,\n",
    "    \"CONV_LAYERS\": 4,\n",
    "    \"N_FILTERS\": 8,\n",
    "    \"KERNEL_SIZE\": (3,3),\n",
    "    \"EMBEDDING_SIZE\": 16,\n",
    "    \"VECTOR_SIZE\": 16,\n",
    "    \"DROPOUT\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844f412-7fa1-495e-ae50-ac86a608fcca",
   "metadata": {},
   "source": [
    "## **Initialize WANDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c80150-55e9-4bc9-b00d-db5beb1cdb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mall-off-nothing\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kiern/.netrc\n",
      "C:\\Users\\kiern\\anaconda3\\envs\\deep-clustering-analysis\\lib\\site-packages\\IPython\\html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kmcguigan/deep-clustering-evaluation/runs/6tu8m480\" target=\"_blank\">earnest-deluge-83</a></strong> to <a href=\"https://wandb.ai/kmcguigan/deep-clustering-evaluation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from secrets import WANDB\n",
    "wandb.login(key=WANDB)\n",
    "run = wandb.init(project=\"deep-clustering-evaluation\", entity=\"kmcguigan\", group=\"cosface-model\", config=config, job_type=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17c5e8-bec3-4708-9ba8-e8108d6d2e65",
   "metadata": {},
   "source": [
    "## **Loading Data**\n",
    "\n",
    "### **Load the presplit data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f757c27-97d6-422f-a677-d1e4380075b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (50000, 28, 28, 1) Val data shape: (10000, 28, 28, 1) Test data shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "with open('data/train.npy', mode='rb') as infile:\n",
    "    X_train = np.load(infile, allow_pickle=True)\n",
    "    y_train = np.load(infile, allow_pickle=True)\n",
    "\n",
    "with open('data/val.npy', mode='rb') as infile:\n",
    "    X_val = np.load(infile, allow_pickle=True)\n",
    "    y_val = np.load(infile, allow_pickle=True)\n",
    "\n",
    "with open('data/test.npy', mode='rb') as infile:\n",
    "    X_test = np.load(infile, allow_pickle=True)\n",
    "    y_test = np.load(infile, allow_pickle=True)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape} Val data shape: {X_val.shape} Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e815553c-168d-48fb-9296-1c0354dca281",
   "metadata": {},
   "source": [
    "### **Create a data generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73ccb7bb-37b6-4a6e-938d-0c98ed28e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(X, y):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(({\"images\":X,\"labels\":y},y))\n",
    "    ds = ds.cache().shuffle(X.shape[0]+1).batch(config[\"BATCH_SIZE\"]).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = to_dataset(X_train, y_train)\n",
    "val_ds = to_dataset(X_val, y_val)\n",
    "test_ds = to_dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50728d79-19ef-4dec-93ef-ae8dbc0632a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Define Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8de6f76-eba7-4244-8b9e-46a385795990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pairwise_distance(embeddings, squared=False):\n",
    "    dot = tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "    square_norm = tf.linalg.diag_part(dot)\n",
    "    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot + tf.expand_dims(square_norm, 0)\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "    if(not squared):\n",
    "        mask = tf.cast(tf.equal(distances, 0.0), tf.float32)\n",
    "        distances = distances + mask * 1e-16\n",
    "        distances = tf.sqrt(distances)\n",
    "        distances = distances * (1.0 - mask)\n",
    "    return distances\n",
    "\n",
    "def angular_distances(embeddings):\n",
    "    embeddings = tf.math.l2_normalize(embeddings, axis=-1)\n",
    "    angular_distances = 1 - tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "    angular_distances = tf.maximum(angular_distances, 0.0)\n",
    "    mask_offdiag = tf.ones_like(angular_distances) - tf.linalg.diag(tf.ones([tf.shape(angular_distances)[0]]))\n",
    "    angular_distances = tf.math.multiply(angular_distances, mask_offdiag)\n",
    "    return angular_distances\n",
    "\n",
    "def apply_metric(embeddings, labels, metric):\n",
    "    adj = tf.equal(labels, tf.transpose(labels))\n",
    "    adj_not = tf.math.logical_not(adj)\n",
    "    adj = tf.cast(adj, tf.float32) - tf.linalg.diag(tf.ones([tf.shape(labels)[0]]))\n",
    "    adj_not = tf.cast(adj_not, tf.float32)\n",
    "    distances = metric(embeddings)\n",
    "    pos_dist = tf.math.multiply(distances, adj)\n",
    "    neg_dist = tf.math.multiply(distances, adj_not)\n",
    "    pos_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(pos_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    neg_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(neg_dist, mask=tf.math.equal(adj_not, 1.0)))\n",
    "    return pos_dist_mean, neg_dist_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a0c220-c7a8-4f85-bcd6-0b15e0843fec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def positive_distance(labels, embeddings):\n",
    "    labels = tf.expand_dims(labels,-1)\n",
    "    adj = tf.equal(labels, tf.transpose(labels))\n",
    "    adj = tf.cast(adj, tf.float32) - tf.linalg.diag(tf.ones([tf.shape(labels)[0]]))\n",
    "    distances = pairwise_distance(embeddings)\n",
    "    pos_dist = tf.math.multiply(distances, adj)\n",
    "    pos_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(pos_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    return pos_dist_mean\n",
    "\n",
    "def negative_distance(labels, embeddings):\n",
    "    labels = tf.expand_dims(labels,-1)\n",
    "    adj_not = tf.math.logical_not(tf.equal(labels, tf.transpose(labels)))\n",
    "    adj_not_float = tf.cast(adj_not, tf.float32)\n",
    "    distances = pairwise_distance(embeddings)\n",
    "    neg_dist = tf.math.multiply(distances, adj_not_float)\n",
    "    neg_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(neg_dist, mask=adj_not))\n",
    "    return neg_dist_mean\n",
    "\n",
    "def positive_angular(labels, embeddings):\n",
    "    labels = tf.expand_dims(labels,-1)\n",
    "    adj = tf.equal(labels, tf.transpose(labels))\n",
    "    adj = tf.cast(adj, tf.float32) - tf.linalg.diag(tf.ones([tf.shape(labels)[0]]))\n",
    "    distances = angular_distances(embeddings)\n",
    "    pos_dist = tf.math.multiply(distances, adj)\n",
    "    pos_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(pos_dist, mask=tf.math.equal(adj, 1.0)))\n",
    "    return pos_dist_mean\n",
    "\n",
    "def negative_angular(labels, embeddings):\n",
    "    labels = tf.expand_dims(labels,-1)\n",
    "    adj = tf.math.logical_not(tf.equal(labels, tf.transpose(labels)))\n",
    "    adj_float = tf.cast(adj, tf.float32)\n",
    "    distances = angular_distances(embeddings)\n",
    "    neg_dist = tf.math.multiply(distances, adj_float)\n",
    "    neg_dist_mean = tf.reduce_mean(tf.ragged.boolean_mask(neg_dist, mask=adj))\n",
    "    return neg_dist_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658690a6-4672-4ac7-9dd6-593f664fc2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lr_callback(plot=False, batch_size=config['BATCH_SIZE'], epochs=config['EPOCHS']):\n",
    "    lr_start   = config['LR']\n",
    "    lr_max     = config['LR'] * 5 * batch_size  \n",
    "    lr_min     = config['LR']\n",
    "    lr_ramp_ep = 4\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.9\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        return lr\n",
    "    if(plot):\n",
    "        epochs = list(range(epochs))\n",
    "        learning_rates = [lrfn(x) for x in epochs]\n",
    "        plt.scatter(epochs,learning_rates)\n",
    "        ax = plt.gca()\n",
    "        ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "        plt.show()\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback\n",
    "\n",
    "if(config[\"LR_STYLE\"] == \"SCHEDULE\"):\n",
    "    lr_callback = get_lr_callback(plot=True)\n",
    "elif(config[\"LR_STYLE\"] == \"REDUCE\"):\n",
    "    lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=2)\n",
    "else:\n",
    "    raise Exception(f\"config LR_STYLE {config['LR_STYLE']} is not understood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3dbe77-9b19-4b95-9603-0dda08caf799",
   "metadata": {},
   "source": [
    "## **Create Model**\n",
    "\n",
    "### **Load the pretrained body model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0a7ca00-3ae8-49b7-8c62-e4719798076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "def freeze_BN(model):\n",
    "    # Unfreeze layers while leaving BatchNorm layers frozen\n",
    "    for layer in model.layers:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "            \n",
    "def freeze_none(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b29ab84-7ab6-418c-9cb4-796a9518f7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"body\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 14, 16)        160       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 14, 14, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 8)         1160      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 14, 14, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 14, 14, 16)        144       \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 16)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,408\n",
      "Trainable params: 3,312\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_body(image_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=image_shape)\n",
    "    \n",
    "    def conv_block(layer_inputs, n_filters, kernel_size, **kwargs):\n",
    "        x = tf.keras.layers.Conv2D(n_filters, kernel_size, padding=\"same\", **kwargs)(layer_inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        return x\n",
    "    \n",
    "    x = conv_block(inputs, config[\"FIRST_FILTERS\"], config[\"KERNEL_SIZE\"], strides=2)\n",
    "    for _ in range(config[\"CONV_LAYERS\"]):\n",
    "        x = conv_block(x, config[\"N_FILTERS\"], config[\"KERNEL_SIZE\"])\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(config[\"EMBEDDING_SIZE\"], (1,1), padding=\"same\")(x)\n",
    "    outputs = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"body\")\n",
    "\n",
    "body = create_body(X_train.shape[1:])\n",
    "body.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1174cb-a7a8-4720-955e-11c65845d140",
   "metadata": {},
   "source": [
    "### **Create the head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed0fd5d-79c0-4df1-b840-da463fd856a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 16)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 272\n",
      "Trainable params: 272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_head(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=(input_shape,))\n",
    "    x = tf.keras.layers.Dropout(config[\"DROPOUT\"])(inputs)\n",
    "    x = tf.keras.layers.Dense(config['VECTOR_SIZE'])(x)\n",
    "    outputs = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1))(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"head\")\n",
    "\n",
    "head = create_head(input_shape=config['EMBEDDING_SIZE'])\n",
    "head.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e99d4-050a-4015-b9b8-82802feb884a",
   "metadata": {},
   "source": [
    "### **Create the loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "421bc9bc-303b-44da-8953-be2b59341a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class CosFace(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implementation of CosFace layer. Reference: https://arxiv.org/abs/1801.09414\n",
    "    \n",
    "    Arguments:\n",
    "      num_classes: number of classes to classify\n",
    "      s: scale factor\n",
    "      m: margin\n",
    "      regularizer: weights regularizer\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_classes,\n",
    "                 s=30.0,\n",
    "                 m=0.35,\n",
    "                 regularizer=None,\n",
    "                 name='cosface',\n",
    "                 **kwargs):\n",
    "\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = float(s)\n",
    "        self.m = float(m)\n",
    "        self.regularizer = regularizer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embedding_shape, label_shape = input_shape\n",
    "        self.w = self.add_weight(shape=(embedding_shape[-1], self.n_classes),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  regularizer=self.regularizer)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'regularizer': self.regularizer\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        During training, requires 2 inputs: embedding (after backbone+pool+dense),\n",
    "        and ground truth labels. The labels should be sparse (and use\n",
    "        sparse_categorical_crossentropy as loss).\n",
    "        \"\"\"\n",
    "        embedding, label = inputs\n",
    "\n",
    "        # Squeezing is necessary for Keras. It expands the dimension to (n, 1)\n",
    "        label = tf.reshape(label, [-1], name='label_shape_correction')\n",
    "\n",
    "        # Normalize features and weights and compute dot product\n",
    "        x = tf.nn.l2_normalize(embedding, axis=1, name='normalize_prelogits')\n",
    "        w = tf.nn.l2_normalize(self.w, axis=0, name='normalize_weights')\n",
    "        cosine_sim = tf.squeeze(tf.matmul(tf.expand_dims(x,1), w, name='cosine_similarity'), axis=1, name=\"reduce_matmul_dims\")\n",
    "        if not training:\n",
    "            # We don't have labels if we're not in training mode\n",
    "            return tf.math.multiply(self.s, cosine_sim)\n",
    "        else:\n",
    "            target_logits = cosine_sim - self.m\n",
    "            logits = tf.math.multiply(cosine_sim, tf.expand_dims((1 - label),1)) + tf.math.multiply(target_logits, tf.expand_dims(label,1))\n",
    "            return tf.math.multiply(self.s, logits)\n",
    "        \n",
    "# class CosFace(tf.keras.layers.Layer):\n",
    "#     \"\"\"https://github.com/4uiiurz1/keras-arcface/blob/master/metrics.py\n",
    "#     \"\"\"\n",
    "#     def __init__(self, n_classes=10, s=30.0, m=0.35, regularizer=None, **kwargs):\n",
    "#         super(CosFace, self).__init__(**kwargs)\n",
    "#         self.n_classes = n_classes\n",
    "#         self.s = s\n",
    "#         self.m = m\n",
    "#         # self.regularizer = regularizers.get(regularizer)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         super(CosFace, self).build(input_shape[0])\n",
    "#         self.W = self.add_weight(name='W',\n",
    "#                                 shape=(input_shape[0][-1], self.n_classes),\n",
    "#                                 initializer='glorot_uniform',\n",
    "#                                 trainable=True)#,\n",
    "#                                 # regularizer=self.regularizer)\n",
    "#     def get_config(self):\n",
    "\n",
    "#         config = super().get_config().copy()\n",
    "#         config.update({\n",
    "#             'n_classes': self.n_classes,\n",
    "#             's': self.s,\n",
    "#             'm': self.m,\n",
    "#             # 'ls_eps': self.ls_eps,\n",
    "#             # 'easy_margin': self.easy_margin,\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x, y = inputs\n",
    "#         c = K.shape(x)[-1]\n",
    "#         # normalize feature\n",
    "#         x = tf.nn.l2_normalize(x, axis=1)\n",
    "#         # normalize weights\n",
    "#         W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "#         # dot product\n",
    "#         logits = tf.matmul(x, W)\n",
    "#         # add margin\n",
    "#         target_logits = logits - self.m\n",
    "#         #\n",
    "#         logits = logits * (1 - y) + target_logits * y\n",
    "#         # feature re-scale\n",
    "#         logits = logits * self.s\n",
    "#         return logits\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return (None, self.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb42551-d62c-4b24-8d03-aefb7643ca12",
   "metadata": {},
   "source": [
    "### **Create the full model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f227d2ef-711a-4894-949a-6dc15f779d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " images (InputLayer)            [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " body (Functional)              (None, 16)           3408        ['images[0][0]']                 \n",
      "                                                                                                  \n",
      " head (Functional)              (None, 16)           272         ['body[0][0]']                   \n",
      "                                                                                                  \n",
      " labels (InputLayer)            [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " cosface (CosFace)              (None, 10)           160         ['head[0][0]',                   \n",
      "                                                                  'labels[0][0]']                 \n",
      "                                                                                                  \n",
      " softmax (Softmax)              (None, 10)           0           ['cosface[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,840\n",
      "Trainable params: 3,744\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(image_size, nclasses):\n",
    "    inputs = tf.keras.layers.Input(shape=image_size, name=\"images\")\n",
    "    labels = tf.keras.layers.Input(shape=(), name=\"labels\")\n",
    "    x = body(inputs)\n",
    "    embeddings = head(x)\n",
    "    x = CosFace(nclasses)([embeddings, labels])\n",
    "    outputs = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "    model = tf.keras.models.Model(inputs=[inputs, labels], outputs=outputs)\n",
    "    embedding_model = tf.keras.models.Model(inputs=inputs, outputs=embeddings)\n",
    "    \n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy(), tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3)]\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=config['LR'])\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model, embedding_model\n",
    "\n",
    "model, embedding_model = get_model(config[\"IMAGE_SIZE\"], nclasses=10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1aa0f1-9e55-45a1-bacc-94fcca30479f",
   "metadata": {},
   "source": [
    "## **Evaluate Models Initial Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8db1a1ad-7879-4346-8ff3-f6bb878fee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster_accuracy(X, y):\n",
    "    embeddings = embedding_model.predict(X)\n",
    "    kmeans = KMeans(n_clusters=10, random_state=123)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    label_mappings = {}\n",
    "    for label in np.unique(labels):\n",
    "        values, counts = np.unique(y[np.where(labels==label)], return_counts=True)\n",
    "        label_mappings[label] = values[np.argmax(counts)]\n",
    "    print(label_mappings)\n",
    "    \n",
    "    map_labels = np.vectorize(lambda x: label_mappings[x])\n",
    "    mapped_labels = map_labels(labels)\n",
    "    return accuracy_score(y.reshape((-1,1)), mapped_labels.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6df77108-29e2-422b-98fc-a7a16c4181f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4, 1: 8, 2: 1, 3: 3, 4: 4, 5: 8, 6: 9, 7: 1, 8: 0, 9: 3}\n",
      "0.278\n"
     ]
    }
   ],
   "source": [
    "acc = kmeans_cluster_accuracy(X_test, y_test)\n",
    "print(acc)\n",
    "run.log({'test/init-test-clustering-accuracy': acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ec03f3c-f495-43e4-9aa5-9c70dcdc8438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4, 1: 1, 2: 9, 3: 1, 4: 8, 5: 3, 6: 3, 7: 0, 8: 9, 9: 4}\n",
      "0.2837\n"
     ]
    }
   ],
   "source": [
    "acc = kmeans_cluster_accuracy(X_val, y_val)\n",
    "print(acc)\n",
    "run.log({'test/init-val-clustering-accuracy': acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd4e80c-a7bc-4774-a544-f356b25a6d69",
   "metadata": {},
   "source": [
    "## **Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b92365b3-811f-4541-894c-4de416e7cddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "782/782 [==============================] - 20s 24ms/step - loss: 0.6911 - sparse_categorical_accuracy: 0.8164 - sparse_top_k_categorical_accuracy: 0.9339 - val_loss: 0.3378 - val_sparse_categorical_accuracy: 0.8999 - val_sparse_top_k_categorical_accuracy: 0.9805 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "782/782 [==============================] - 19s 25ms/step - loss: 0.1671 - sparse_categorical_accuracy: 0.9506 - sparse_top_k_categorical_accuracy: 0.9923 - val_loss: 0.2075 - val_sparse_categorical_accuracy: 0.9332 - val_sparse_top_k_categorical_accuracy: 0.9919 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.1266 - sparse_categorical_accuracy: 0.9620 - sparse_top_k_categorical_accuracy: 0.9946 - val_loss: 0.1571 - val_sparse_categorical_accuracy: 0.9519 - val_sparse_top_k_categorical_accuracy: 0.9935 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.1108 - sparse_categorical_accuracy: 0.9657 - sparse_top_k_categorical_accuracy: 0.9960 - val_loss: 0.1252 - val_sparse_categorical_accuracy: 0.9595 - val_sparse_top_k_categorical_accuracy: 0.9955 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.0985 - sparse_categorical_accuracy: 0.9698 - sparse_top_k_categorical_accuracy: 0.9964 - val_loss: 0.1814 - val_sparse_categorical_accuracy: 0.9440 - val_sparse_top_k_categorical_accuracy: 0.9926 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.0907 - sparse_categorical_accuracy: 0.9716 - sparse_top_k_categorical_accuracy: 0.9968 - val_loss: 0.2011 - val_sparse_categorical_accuracy: 0.9406 - val_sparse_top_k_categorical_accuracy: 0.9925 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.0803 - sparse_categorical_accuracy: 0.9751 - sparse_top_k_categorical_accuracy: 0.9973 - val_loss: 0.2377 - val_sparse_categorical_accuracy: 0.9284 - val_sparse_top_k_categorical_accuracy: 0.9907 - lr: 9.0000e-04\n",
      "Epoch 8/30\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.0762 - sparse_categorical_accuracy: 0.9762 - sparse_top_k_categorical_accuracy: 0.9976 - val_loss: 0.0889 - val_sparse_categorical_accuracy: 0.9726 - val_sparse_top_k_categorical_accuracy: 0.9980 - lr: 9.0000e-04\n",
      "Epoch 9/30\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.0739 - sparse_categorical_accuracy: 0.9772 - sparse_top_k_categorical_accuracy: 0.9976 - val_loss: 0.1294 - val_sparse_categorical_accuracy: 0.9620 - val_sparse_top_k_categorical_accuracy: 0.9954 - lr: 9.0000e-04\n",
      "Epoch 10/30\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.0692 - sparse_categorical_accuracy: 0.9779 - sparse_top_k_categorical_accuracy: 0.9981 - val_loss: 0.1150 - val_sparse_categorical_accuracy: 0.9672 - val_sparse_top_k_categorical_accuracy: 0.9966 - lr: 9.0000e-04\n",
      "Epoch 11/30\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.0644 - sparse_categorical_accuracy: 0.9795 - sparse_top_k_categorical_accuracy: 0.9983 - val_loss: 0.1194 - val_sparse_categorical_accuracy: 0.9687 - val_sparse_top_k_categorical_accuracy: 0.9958 - lr: 8.1000e-04\n",
      "Epoch 12/30\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.0622 - sparse_categorical_accuracy: 0.9797 - sparse_top_k_categorical_accuracy: 0.9984 - val_loss: 0.0865 - val_sparse_categorical_accuracy: 0.9738 - val_sparse_top_k_categorical_accuracy: 0.9973 - lr: 8.1000e-04\n",
      "Epoch 13/30\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.0603 - sparse_categorical_accuracy: 0.9808 - sparse_top_k_categorical_accuracy: 0.9984 - val_loss: 0.1873 - val_sparse_categorical_accuracy: 0.9482 - val_sparse_top_k_categorical_accuracy: 0.9938 - lr: 8.1000e-04\n",
      "Epoch 14/30\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.0596 - sparse_categorical_accuracy: 0.9799 - sparse_top_k_categorical_accuracy: 0.9985 - val_loss: 0.2279 - val_sparse_categorical_accuracy: 0.9348 - val_sparse_top_k_categorical_accuracy: 0.9919 - lr: 8.1000e-04\n",
      "Epoch 15/30\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.0525 - sparse_categorical_accuracy: 0.9833 - sparse_top_k_categorical_accuracy: 0.9988 - val_loss: 0.1525 - val_sparse_categorical_accuracy: 0.9553 - val_sparse_top_k_categorical_accuracy: 0.9954 - lr: 7.2900e-04\n",
      "Epoch 16/30\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.0526 - sparse_categorical_accuracy: 0.9833 - sparse_top_k_categorical_accuracy: 0.9988 - val_loss: 0.1890 - val_sparse_categorical_accuracy: 0.9454 - val_sparse_top_k_categorical_accuracy: 0.9925 - lr: 7.2900e-04\n"
     ]
    }
   ],
   "source": [
    "stopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "hist = model.fit(train_ds,\n",
    "                 validation_data=val_ds,\n",
    "                 epochs=config[\"EPOCHS\"],\n",
    "                 callbacks=[stopper, lr_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53f5eea3-2fa5-46e9-8b99-1a96c7ff8a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 5ms/step - loss: 0.0691 - sparse_categorical_accuracy: 0.9787 - sparse_top_k_categorical_accuracy: 0.9979\n"
     ]
    }
   ],
   "source": [
    "ev = model.evaluate(test_ds, return_dict=True)\n",
    "log_dict = {f'test/{met}': val for met, val in ev.items()}\n",
    "run.log(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b35cf28f-fcab-4bf8-929f-c773b3841754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 6, 2: 3, 3: 9, 4: 4, 5: 8, 6: 0, 7: 2, 8: 7, 9: 5}\n",
      "0.9756\n"
     ]
    }
   ],
   "source": [
    "acc = kmeans_cluster_accuracy(X_test, y_test)\n",
    "print(acc)\n",
    "run.log({'test/test-clustering-accuracy': acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55f26e60-60f3-4c55-ac48-2421d4370926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 8, 2: 3, 3: 0, 4: 7, 5: 6, 6: 2, 7: 9, 8: 5, 9: 4}\n",
      "0.9683\n"
     ]
    }
   ],
   "source": [
    "acc = kmeans_cluster_accuracy(X_val, y_val)\n",
    "print(acc)\n",
    "run.log({'test/val-clustering-accuracy': acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec24725-54c8-4d99-aeb2-f3f0285ebdad",
   "metadata": {},
   "source": [
    "## **Evalueate Separation on Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84787f9b-617d-4a99-8199-7ad3a759585f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive_distance 0.35135334730148315 negative_distance 0.9781956672668457 positive_angular 0.07162502408027649 negative_angular 0.49920883774757385\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_model.predict(X_test)\n",
    "pd = positive_distance(y_test, embeddings)\n",
    "nd = negative_distance(y_test, embeddings)\n",
    "pa = positive_angular(y_test, embeddings)\n",
    "na = negative_angular(y_test, embeddings)\n",
    "print(f\"positive_distance {pd} negative_distance {nd} positive_angular {pa} negative_angular {na}\")\n",
    "run.log({'test/positive_distance': pd})\n",
    "run.log({'test/negative_distance': nd})\n",
    "run.log({'test/positive_angular': pa})\n",
    "run.log({'test/negative_angular': na})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08655727-f046-4460-9197-6b86292904b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22052... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>loss</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▅▅▅▅▃▃▃▃▁▁</td></tr><tr><td>sparse_categorical_accuracy</td><td>▁▇▇▇▇███████████</td></tr><tr><td>sparse_top_k_categorical_accuracy</td><td>▁▇██████████████</td></tr><tr><td>test/init-test-clustering-accuracy</td><td>▁</td></tr><tr><td>test/init-val-clustering-accuracy</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/negative_angular</td><td>▁</td></tr><tr><td>test/negative_distance</td><td>▁</td></tr><tr><td>test/positive_angular</td><td>▁</td></tr><tr><td>test/positive_distance</td><td>▁</td></tr><tr><td>test/sparse_categorical_accuracy</td><td>▁</td></tr><tr><td>test/sparse_top_k_categorical_accuracy</td><td>▁</td></tr><tr><td>test/test-clustering-accuracy</td><td>▁</td></tr><tr><td>test/val-clustering-accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▄▄▅▁▂▂▂▁▄▅▃▄</td></tr><tr><td>val_sparse_categorical_accuracy</td><td>▁▄▆▇▅▅▄█▇▇██▆▄▆▅</td></tr><tr><td>val_sparse_top_k_categorical_accuracy</td><td>▁▆▆▇▆▆▅█▇▇▇█▆▆▇▆</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.08648</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>loss</td><td>0.05255</td></tr><tr><td>lr</td><td>0.00073</td></tr><tr><td>sparse_categorical_accuracy</td><td>0.98326</td></tr><tr><td>sparse_top_k_categorical_accuracy</td><td>0.99876</td></tr><tr><td>test/init-test-clustering-accuracy</td><td>0.278</td></tr><tr><td>test/init-val-clustering-accuracy</td><td>0.2837</td></tr><tr><td>test/loss</td><td>0.06913</td></tr><tr><td>test/negative_angular</td><td>0.49921</td></tr><tr><td>test/negative_distance</td><td>0.9782</td></tr><tr><td>test/positive_angular</td><td>0.07163</td></tr><tr><td>test/positive_distance</td><td>0.35135</td></tr><tr><td>test/sparse_categorical_accuracy</td><td>0.9787</td></tr><tr><td>test/sparse_top_k_categorical_accuracy</td><td>0.9979</td></tr><tr><td>test/test-clustering-accuracy</td><td>0.9756</td></tr><tr><td>test/val-clustering-accuracy</td><td>0.9683</td></tr><tr><td>val_loss</td><td>0.18901</td></tr><tr><td>val_sparse_categorical_accuracy</td><td>0.9454</td></tr><tr><td>val_sparse_top_k_categorical_accuracy</td><td>0.9925</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-deluge-83</strong>: <a href=\"https://wandb.ai/kmcguigan/deep-clustering-evaluation/runs/6tu8m480\" target=\"_blank\">https://wandb.ai/kmcguigan/deep-clustering-evaluation/runs/6tu8m480</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220325_171414-6tu8m480\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a2f25-1309-406d-9522-e250c08176e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
